{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#artificial-neural-networks-portfolio-mateus-marinheiro","title":"Artificial Neural Networks Portfolio - Mateus Marinheiro","text":"<p>Welcome to my portfolio for the Artificial Neural Networks and Deep Learning course. Here I will document all exercises and the final project developed throughout the semester.  </p>"},{"location":"#deliveries","title":"Deliveries","text":"<ul> <li> Exercise 1 \u2014 Data (delivered on 05/09/2025) </li> <li> Exercise 2 \u2014 Perceptron (delivered on 14/09/2025)</li> <li> Exercise 3 \u2014 MLP  (delivered on 21/09/2025)</li> <li> Exercise 4 \u2014 Metrics  </li> <li> Final Project </li> </ul>"},{"location":"#organization-of-this-portfolio","title":"Organization of this Portfolio","text":"<ul> <li>Each Exercise has its own page with:</li> <li>Problem statement  </li> <li>Implementation (code snippets)  </li> <li>Results (plots, tables, metrics)  </li> <li> <p>Discussion (analysis and conclusions)  </p> </li> <li> <p>The Final Project will summarize the main learnings and present a complete application of neural networks.</p> </li> </ul>"},{"location":"exercicios/ex1-data/main/","title":"1. Data","text":""},{"location":"exercicios/ex1-data/main/#1-data","title":"1. Data","text":""},{"location":"exercicios/ex1-data/main/#exercise-1-exploring-class-separability-in-2d","title":"Exercise 1 \u2013 Exploring Class Separability in 2D","text":""},{"location":"exercicios/ex1-data/main/#1-data-generation","title":"1) Data Generation","text":"<p>Create a synthetic 2D dataset with 400 samples, evenly split across four classes (100 each). Each class is sampled independently from a Gaussian (normal) distribution with its own mean and per-axis standard deviations:</p> <ul> <li>Class 0: mean \u2248 (2, 3), std \u2248 (0.8, 2.5)</li> <li>Class 1: mean \u2248 (5, 6), std \u2248 (1.2, 1.9)</li> <li>Class 2: mean \u2248 (8, 1), std \u2248 (0.9, 0.9)</li> <li>Class 3: mean \u2248 (15, 4), std \u2248 (0.5, 2.0)</li> </ul> <p>What this means. For each class \\(c\\), we draw 100 points \\((x_1, x_2)\\) where: - \\(x_1 \\sim \\mathcal{N}(\\mu_{c,1}, \\sigma_{c,1})\\) - \\(x_2 \\sim \\mathcal{N}(\\mu_{c,2}, \\sigma_{c,2})\\)</p> <p>This produces four \u201cclouds\u201d of points centered around their means, with vertical/horizontal spread governed by the standard deviations.</p> Data Generation Code <pre><code>import numpy as np\nimport pandas as pd\n\n# Class definitions\nparams = {\n    0: {\"mean\": [2, 3],   \"std\": [0.8, 2.5]},\n    1: {\"mean\": [5, 6],   \"std\": [1.2, 1.9]},\n    2: {\"mean\": [8, 1],   \"std\": [0.9, 0.9]},\n    3: {\"mean\": [15, 4],  \"std\": [0.5, 2.0]},\n}\n\nN = 100\nrng = np.random.default_rng()\n\n# Create individual DataFrames\ndfs = []\nfor cls, p in params.items():\n    x1 = rng.normal(p[\"mean\"][0], p[\"std\"][0], N)\n    x2 = rng.normal(p[\"mean\"][1], p[\"std\"][1], N)\n    df_cls = pd.DataFrame({\"class\": cls, \"x1\": x1, \"x2\": x2})\n    dfs.append(df_cls)\n\n# Concatenate all DataFrames\ndf = pd.concat(dfs, ignore_index=True)\nprint(df.head(10))\n</code></pre>"},{"location":"exercicios/ex1-data/main/#2-plot-the-data","title":"2) Plot the Data","text":"<p>Figure 1 \u2014 2D scatter plot of the generated dataset. Each color represents a different class.</p>"},{"location":"exercicios/ex1-data/main/#3-analyze-and-draw-boundaries","title":"3) Analyze and Draw Boundaries","text":""},{"location":"exercicios/ex1-data/main/#a-scatter-plot-analysis","title":"a. Scatter Plot Analysis","text":"<p>By examining the scatter plot, it is possible to identify the following patterns:</p> <ul> <li>Class 0 (blue) forms a cluster on the left side, centered around (2,3). It shows a high vertical spread, which causes partial overlap with the lower points of Class 1.  </li> <li>Class 1 (orange) is located slightly to the right and above Class 0, centered near (5,6). Due to its vertical dispersion, it overlaps not only with the upper boundary of Class 0 but also approaches the top region of Class 2.  </li> <li>Class 2 (green) is concentrated near (8,1), with a smaller variance, making it more compact. However, it still lies close to the bottom of Class 1, leading to a noticeable boundary interaction.  </li> <li>Class 3 (red) is clearly separated on the far right, around (15,4). This class does not show any significant overlap with the others, making it the easiest to isolate.  </li> </ul> <p>In summary, Classes 0, 1, and 2 show areas of overlap due to their vertical spread and proximity along the x-axis, while Class 3 remains well isolated from the others.</p>"},{"location":"exercicios/ex1-data/main/#b-linear-separability","title":"b. Linear Separability","text":"<p>A single linear boundary cannot separate all four classes. As illustrated by the sketched straight lines (purple for Class 0\u20131, yellow for Class 1\u20132, and a gray vertical line isolating Class 3), we already need multiple linear cuts to approximate the partitions. Even then, misclassifications remain:</p> <ul> <li>Class 3 can be isolated with a vertical line on the far right.</li> <li>Classes 0 and 1 overlap vertically; any straight line will mislabel points near the central band.</li> <li>Classes 1 and 2 intersect around the lower edge of Class 1 and the upper edge of Class 2; a diagonal line reduces but does not eliminate errors.</li> </ul> <p>Therefore, while straight lines help illustrate partial separation, linear decision boundaries alone cannot perfectly separate all classes due to the overlap and shaped spreads of Classes 0, 1, and 2.</p>"},{"location":"exercicios/ex1-data/main/#c-decision-boundaries-sketch","title":"c. Decision boundaries (sketch)","text":"<p>Below I include the sketch of decision boundaries that a trained neural network might learn. In this illustration I use three straight lines: a diagonal separating Classes 0\u20131, another diagonal separating Classes 1\u20132, and a vertical line isolating Class 3 on the far right.</p> <p></p> <p>Figure 2 \u2014 Three straight separators: 0\u20131 (diagonal), 1\u20132 (diagonal), and 2\u20133 (vertical).</p> Code used to draw the sketch (replace coordinates with your final ones) <pre><code>plt.figure(figsize=(10, 6))\n\n# Scatter plot of the classes\nsns.scatterplot(data=df, x=\"x1\", y=\"x2\", hue=\"classe\", palette=\"tab10\")\n\n# Manually sketched straight lines (approximate coordinates)\nline1_x = [2, 5]\nline1_y = [10, -4]\n\nline2_x = [4, 10]\nline2_y = [-0.2, 8]\n\nline3_x = [13, 13]\nline3_y = [-2, 10]\n\n# Plot the lines\nplt.plot(line1_x, line1_y, color=\"purple\", linewidth=2, linestyle=\"--\", label=\"0\u20131 division\")\nplt.plot(line2_x, line2_y, color=\"gold\", linewidth=2, linestyle=\"--\", label=\"1\u20132 division\")\nplt.plot(line3_x, line3_y, color=\"gray\", linewidth=2, linestyle=\"--\", label=\"2\u20133 division\")\n\n# Styling\nplt.title(\"Scatter Plot with Sketched Decision Boundaries\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend(title=\"Class\", loc=\"lower right\")\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"exercicios/ex1-data/main/#exercise-2-non-linearity-in-higher-dimensions","title":"Exercise 2 \u2014 Non-Linearity in Higher Dimensions","text":""},{"location":"exercicios/ex1-data/main/#1-data-generation_1","title":"1) Data Generation","text":"<p>Goal. Create a 5-dimensional dataset with two classes (A and B), each containing 500 samples. The samples are drawn from multivariate Gaussian distributions with different means and covariance matrices:</p> <ul> <li>Class A: centered around the origin (0, 0, 0, 0, 0) with moderate positive correlations among features.  </li> <li>Class B: centered at (1.5, 1.5, 1.5, 1.5, 1.5) with slightly different correlations and variances.  </li> </ul> <p>This setup produces two overlapping but distinguishable high-dimensional clusters, which makes linear separation difficult.</p> Data Generation Code <pre><code>import numpy as np\nimport pandas as pd\n\n# Parameters for Class A\nmu_A = [0, 0, 0, 0, 0]\nSigma_A = [\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0],\n]\n\n# Parameters for Class B\nmu_B = [1.5, 1.5, 1.5, 1.5, 1.5]\nSigma_B = [\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.6, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5],\n]\n\n# Generate 500 samples for each class\nXA = rng.multivariate_normal(mu_A, Sigma_A, size=500)\nXB = rng.multivariate_normal(mu_B, Sigma_B, size=500)\n\n# Merge into a single dataset\nX = np.vstack([XA, XB])\ny = np.array([0]*500 + [1]*500)  # 0 = Class A, 1 = Class B\n\n# Create a DataFrame (Excel-style table)\ncols = [f\"f{i}\" for i in range(1, 6)]\ndf = pd.DataFrame(X, columns=cols)\ndf[\"class\"] = y\n\nprint(df.head())\n</code></pre>"},{"location":"exercicios/ex1-data/main/#2-data-visualization-dimensionality-reduction-with-pca","title":"2) Data Visualization (Dimensionality Reduction with PCA)","text":"<p>Since the dataset lives in a 5-dimensional space, direct visualization is not possible. To explore the separability of the two classes, we apply Principal Component Analysis (PCA) to project the standardized data into two principal components (PC1 and PC2).</p> Code for Dimensionality Reduction and Visualization <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Step 1 \u2014 Standardize features (mean = 0, std = 1 per column)\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n\n# Step 2 \u2014 PCA projection to 2 dimensions\npca = PCA(n_components=2, random_state=42)\nZ = pca.fit_transform(X_std)\n\nprint(\"Explained variance by PC1 and PC2:\",\n      np.round(pca.explained_variance_ratio_, 4))\nprint(\"Total variance explained (PC1+PC2):\",\n      np.round(pca.explained_variance_ratio_.sum(), 4))\n\n# Step 3 \u2014 Scatter plot of projected data\ncolors = {0: \"#1f77b4\", 1: \"#d62728\"}  # Blue = Class A, Red = Class B\nlabels = {0: \"Class A\", 1: \"Class B\"}\n\nplt.figure(figsize=(8, 6))\nfor c in (0, 1):\n    mask = (y == c)\n    plt.scatter(Z[mask, 0], Z[mask, 1],\n                s=18, alpha=0.75,\n                label=labels[c],\n                c=colors[c])\nplt.xlabel(\"Principal Component 1 (PC1)\")\nplt.ylabel(\"Principal Component 2 (PC2)\")\nplt.title(\"PCA Projection of 5D Data onto First Two Principal Components\")\nplt.legend(title=\"Class\")\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Figure 3 \u2014 PCA projection of the 5D dataset into 2D. Each point corresponds to one sample, colored by class (blue = Class A, red = Class B).</p>"},{"location":"exercicios/ex1-data/main/#3-analysis-of-the-pca-projection","title":"3) Analysis of the PCA Projection","text":""},{"location":"exercicios/ex1-data/main/#a-relationship-between-the-two-classes","title":"a. Relationship Between the Two Classes","text":"<p>The scatter plot shows the dataset after projection into the first two principal components (PC1 and PC2). Some clear patterns can be observed:</p> <ul> <li>Class A (blue) is concentrated on the left-hand side of the PC1 axis, although it spreads vertically along PC2.  </li> <li>Class B (red) is mostly located on the right-hand side, also with some vertical dispersion.  </li> <li>The two classes are not completely isolated \u2014 there is a noticeable overlapping region around the center (PC1 \u2248 0), where samples from both classes mix.  </li> </ul> <p>This indicates that while PCA successfully separates the clusters to some extent, the data distributions remain partially entangled.</p>"},{"location":"exercicios/ex1-data/main/#b-linear-separability_1","title":"b. Linear Separability","text":"<p>From the 2D projection, it becomes clear that a single linear boundary (for example, a vertical cut around PC1 = 0) could reduce the overlap but would not perfectly separate the two classes. Many points from Class A extend into the positive PC1 region, and several Class B points extend into the negative PC1 side.  </p> <p>Thus, the data is not strictly linearly separable. Any linear classifier (such as a perceptron or logistic regression with a single decision boundary) would inevitably misclassify points in the overlapping zone.</p> <p>This type of structure illustrates why simple linear models are insufficient. To capture the curved and complex boundary that better separates the two distributions, we require:</p> <ul> <li>Non-linear activation functions (e.g., tanh, ReLU), which allow the network to learn more flexible boundaries.  </li> <li>Multiple layers, enabling the model to progressively transform the feature space into one where the classes become more separable.  </li> </ul> <p>In other words, a multi-layer neural network with non-linearities is more appropriate for this problem, since it can learn decision regions that are not restricted to simple linear cuts.</p>"},{"location":"exercicios/ex1-data/main/#exercise-3-preparing-real-world-data-for-a-neural-network","title":"Exercise 3 \u2014 Preparing Real-World Data for a Neural Network","text":""},{"location":"exercicios/ex1-data/main/#1-describe-the-data","title":"1) Describe the Data","text":"<p>The dataset comes from the Kaggle competition Spaceship Titanic. Its main objective is to predict whether a passenger was transported to another dimension after the collision of the spaceship Titanic with a spacetime anomaly.  </p> <ul> <li>The target variable is <code>Transported</code>, a boolean column:</li> <li><code>True</code> \u2192 the passenger was transported.</li> <li><code>False</code> \u2192 the passenger was not transported.</li> </ul> <p>The dataset contains 14 columns (including the target). They can be grouped as follows:</p> <ul> <li>Numerical features (continuous values):</li> <li><code>Age</code> \u2192 passenger\u2019s age.</li> <li><code>RoomService</code> \u2192 amount spent in the room service.</li> <li><code>FoodCourt</code> \u2192 amount spent in the food court.</li> <li><code>ShoppingMall</code> \u2192 amount spent in the shopping mall.</li> <li><code>Spa</code> \u2192 amount spent in the spa.</li> <li> <p><code>VRDeck</code> \u2192 amount spent in the VR deck.</p> </li> <li> <p>Categorical features:</p> </li> <li><code>PassengerId</code> \u2192 unique passenger identifier.</li> <li><code>HomePlanet</code> \u2192 the planet where the passenger came from.</li> <li><code>CryoSleep</code> \u2192 whether the passenger was in cryogenic sleep (True/False).</li> <li><code>Cabin</code> \u2192 cabin identifier (deck/number/side).</li> <li><code>Destination</code> \u2192 destination planet.</li> <li><code>VIP</code> \u2192 whether the passenger is a VIP customer (True/False).</li> <li><code>Name</code> \u2192 passenger\u2019s name.</li> </ul> <p>To better understand the dataset, we analyzed the presence of missing values in each column. The following table shows the number of missing entries per feature:</p> <ul> <li>HomePlanet: 201  </li> <li>CryoSleep: 217  </li> <li>Cabin: 199  </li> <li>Destination: 182  </li> <li>Age: 179  </li> <li>VIP: 203  </li> <li>RoomService: 181  </li> <li>FoodCourt: 183  </li> <li>ShoppingMall: 208  </li> <li>Spa: 183  </li> <li>VRDeck: 188  </li> <li>Name: 200  </li> </ul> <p>Columns PassengerId and Transported contain no missing values.</p> <p>This confirms that several categorical (e.g., <code>HomePlanet</code>, <code>CryoSleep</code>) and numerical features (e.g., <code>Age</code>, <code>RoomService</code>) require preprocessing strategies to handle missing values before feeding the dataset into a neural network.</p> Code to Count Missing Values <pre><code>import pandas as pd\n\n# Load dataset\ndf = pd.read_csv(\"train.csv\")\n\n# Count missing values per column\nprint(\"Missing values per column:\")\nprint(df.isnull().sum())\n</code></pre>"},{"location":"exercicios/ex1-data/main/#2-preprocess-the-data","title":"2) Preprocess the Data","text":"<ul> <li>Numerical features (<code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>)   \u2192 Median imputation. Why: the median is robust to outliers (e.g., a few passengers with very high spending). Filling with median avoids biasing the distribution upwards like the mean would.</li> </ul> Code \u2014 Median imputation for numerical features <pre><code>import numpy as np\n\n# Numerical features\nnum_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n\ndf_num = df.copy()\nfor col in num_cols:\n    median_val = df_num[col].median()\n    df_num[col] = df_num[col].fillna(median_val)\n    print(f\"{col}: filled NaNs with median = {median_val}\")\n</code></pre> <ul> <li>Categorical features (<code>HomePlanet</code>, <code>CryoSleep</code>, <code>Destination</code>, <code>VIP</code>)   \u2192 Most frequent (mode) imputation. Why: keeps the dataset consistent by filling with the most common category, while not creating artificial new classes.</li> </ul> Code \u2014 Mode imputation for categorical features <pre><code>cat_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\"]\n\ndf_cat = df_num.copy()\nfor col in cat_cols:\n    mode_val = df_cat[col].mode(dropna=True)[0]\n    df_cat[col] = df_cat[col].fillna(mode_val)\n    print(f\"{col}: filled NaNs with mode = {repr(mode_val)}\")\n</code></pre> <ul> <li>Cabin   \u2192 Split into 3 separate features: <code>Deck</code>, <code>CabinNum</code>, and <code>Side</code>.  </li> <li><code>Deck</code>: letter at the start.  </li> <li><code>CabinNum</code>: numeric part (converted to integer).  </li> <li><code>Side</code>: last letter (e.g., P or S).   After splitting, apply mode imputation to each of the three. Why: splitting provides interpretable components instead of treating \u201cCabin\u201d as a single string.</li> </ul> Code \u2014 Split Cabin and impute <pre><code>df_cabin = df_cat.copy()\n\n# Split Cabin into 3 parts\ncabin_split = df_cabin[\"Cabin\"].str.split(\"/\", expand=True)\ndf_cabin[\"Deck\"] = cabin_split[0]\ndf_cabin[\"CabinNum\"] = pd.to_numeric(cabin_split[1], errors=\"coerce\")\ndf_cabin[\"Side\"] = cabin_split[2]\n\n# Drop original Cabin\ndf_cabin = df_cabin.drop(columns=[\"Cabin\"])\n\n# Impute Deck and Side with mode, CabinNum with median\nfor col in [\"Deck\", \"Side\"]:\n    mode_val = df_cabin[col].mode(dropna=True)[0]\n    df_cabin[col] = df_cabin[col].fillna(mode_val)\n    print(f\"{col}: filled NaNs with mode = {repr(mode_val)}\")\n\ncabin_num_median = df_cabin[\"CabinNum\"].median()\ndf_cabin[\"CabinNum\"] = df_cabin[\"CabinNum\"].fillna(cabin_num_median)\nprint(f\"CabinNum: filled NaNs with median = {cabin_num_median}\")\n</code></pre> <p>After handling missing values in the previous step, our categorical features include:</p> <ul> <li><code>HomePlanet</code> </li> <li><code>CryoSleep</code> </li> <li><code>Destination</code> </li> <li><code>VIP</code> </li> <li><code>Deck</code> </li> <li><code>Side</code></li> </ul> <p>(Note: <code>Cabin</code> was already split into <code>Deck</code>, <code>CabinNum</code>, <code>Side</code>.)</p> <p>We will use <code>pandas.get_dummies()</code> for simplicity. It automatically creates new columns for each category.</p> Code \u2014 One-hot encoding with pandas <pre><code># Assume df_clean is the dataset after missing value handling\ncategorical_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Deck\", \"Side\"]\n\n# One-hot encode categorical columns\ndf_encoded = pd.get_dummies(df_clean, columns=categorical_cols, drop_first=False)\n\nprint(\"Shape after encoding:\", df_encoded.shape)\nprint(\"Sample columns:\", df_encoded.columns[:15].tolist())\n</code></pre> <ul> <li>We use <code>drop_first=False</code> to keep all categories, ensuring no information loss.  </li> <li>This results in new binary columns like <code>HomePlanet_Earth</code>, <code>HomePlanet_Europa</code>, <code>HomePlanet_Mars</code>, etc.</li> </ul> <p>From the dataset (after missing-value handling and encoding), the main numerical columns are:</p> <ul> <li>Demographic &amp; cabin info: <code>Age</code>, <code>CabinNum</code> </li> <li>Spending features: <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code></li> </ul> Code \u2014 Normalize numerical features <pre><code>import numpy as np\n\n# Numeric columns\nnumeric_cols = [\"Age\", \"CabinNum\", \"RoomService\", \"FoodCourt\",\n                \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n\n# Copy DataFrame\ndf_normalized = df_encoded.copy()\n\n# Apply Min-Max normalization to [-1, 1]\nfor col in numeric_cols:\n    col_min = df_normalized[col].min()\n    col_max = df_normalized[col].max()\n\n    df_normalized[col] = 2 * (df_normalized[col] - col_min) / (col_max - col_min) - 1\n\n    print(f\"{col}: min={df_normalized[col].min()}, max={df_normalized[col].max()}\")\n</code></pre> <p>Normalizing inputs to the range [-1, 1] aligns them with the natural output of the <code>tanh</code> activation, which is zero-centered. This keeps data in the central, high-slope region of the function, avoiding saturation near -1 or +1 where gradients vanish. It also balances features on the same scale, preventing large-valued variables from dominating smaller ones. As a result, training becomes more stable and convergence is faster when using <code>tanh</code>.</p>"},{"location":"exercicios/ex1-data/main/#3-visualization-histograms-beforeafter-scaling","title":"3) Visualization \u2014 Histograms Before/After Scaling","text":"<p>Below we compare the distributions before and after normalization for two numerical features: <code>Age</code> and <code>CabinNum</code>.  </p> <p></p> Code \u2014 Histograms (before vs. after normalization) <pre><code>import matplotlib.pyplot as plt\n\n# Expect these DataFrames from your pipeline:\n# df_encoded  -&gt; before normalization\n# df_normalized -&gt; after normalization (numeric in [-1, 1])\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8), constrained_layout=True)\n\n# ---- Age ----\naxes[0, 0].hist(df_encoded[\"Age\"], bins=30, color=\"red\", alpha=0.6, edgecolor=\"black\")\naxes[0, 0].set_title(\"Age \u2014 Before Normalization\")\naxes[0, 0].set_xlabel(\"Value\"); axes[0, 0].set_ylabel(\"Frequency\"); axes[0, 0].grid(alpha=0.25)\n\naxes[0, 1].hist(df_normalized[\"Age\"], bins=30, color=\"blue\", alpha=0.6, edgecolor=\"black\")\naxes[0, 1].set_title(\"Age \u2014 After Normalization\")\naxes[0, 1].set_xlabel(\"Value\"); axes[0, 1].set_ylabel(\"Frequency\"); axes[0, 1].grid(alpha=0.25)\naxes[0, 1].set_xlim(-1, 1)\n\n# Match y-scale for Age row\nymax_age = max(axes[0, 0].get_ylim()[1], axes[0, 1].get_ylim()[1])\naxes[0, 0].set_ylim(0, ymax_age); axes[0, 1].set_ylim(0, ymax_age)\n\n# ---- CabinNum ----\naxes[1, 0].hist(df_encoded[\"CabinNum\"], bins=30, color=\"red\", alpha=0.6, edgecolor=\"black\")\naxes[1, 0].set_title(\"CabinNum \u2014 Before Normalization\")\naxes[1, 0].set_xlabel(\"Value\"); axes[1, 0].set_ylabel(\"Frequency\"); axes[1, 0].grid(alpha=0.25)\n\naxes[1, 1].hist(df_normalized[\"CabinNum\"], bins=30, color=\"blue\", alpha=0.6, edgecolor=\"black\")\naxes[1, 1].set_title(\"CabinNum \u2014 After Normalization\")\naxes[1, 1].set_xlabel(\"Value\"); axes[1, 1].set_ylabel(\"Frequency\"); axes[1, 1].grid(alpha=0.25)\naxes[1, 1].set_xlim(-1, 1)\n\n# Match y-scale for CabinNum row\nymax_cabin = max(axes[1, 0].get_ylim()[1], axes[1, 1].get_ylim()[1])\naxes[1, 0].set_ylim(0, ymax_cabin); axes[1, 1].set_ylim(0, ymax_cabin)\n\nfig.suptitle(\"Distributions Before vs After Normalization\", fontsize=14, y=1.02)\nplt.savefig(\"histograms_age_cabinnum_before_after_simple.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/","title":"2. Perceptron","text":""},{"location":"exercicios/ex2-perceptron/main/#2-perceptron","title":"2. Perceptron","text":""},{"location":"exercicios/ex2-perceptron/main/#exercise-1","title":"Exercise 1","text":""},{"location":"exercicios/ex2-perceptron/main/#1-data-generation","title":"1) Data Generation","text":"<p>We generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. The parameters are:</p> <ul> <li> <p>Class 0:   Mean = [1.5, 1.5]   Covariance matrix = [[0.5, 0], [0, 0.5]]</p> </li> <li> <p>Class 1:   Mean = [5, 5]   Covariance matrix = [[0.5, 0], [0, 0.5]]</p> </li> </ul> <p>These settings ensure that the two classes are mostly linearly separable, since the means are far apart and the variance is small, causing minimal overlap.</p> Code \u2014 Data Generation <pre><code>import numpy as np\n\n# Parameters for Class 0\nmean_class0 = [1.5, 1.5]\ncov_class0 = [[0.5, 0], [0, 0.5]]\n\n# Parameters for Class 1\nmean_class1 = [5, 5]\ncov_class1 = [[0.5, 0], [0, 0.5]]\n\n# Number of samples per class\nn_samples = 1000\n\n# Generate data\nclass0 = np.random.multivariate_normal(mean_class0, cov_class0, n_samples)\nclass1 = np.random.multivariate_normal(mean_class1, cov_class1, n_samples)\n\n# Combine into dataset\nX = np.vstack((class0, class1))\ny = np.hstack((np.zeros(n_samples), np.ones(n_samples)))\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#2-plot-the-data","title":"2) Plot the Data","text":"<p>We now plot the generated dataset to visualize the separation between the two classes. Each point is colored according to its class: blue for Class 0 and red for Class 1.</p> Code \u2014 Scatter Plot <pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(7, 7))\nplt.scatter(class0[:, 0], class0[:, 1], c=\"blue\", alpha=0.5, label=\"Class 0\")\nplt.scatter(class1[:, 0], class1[:, 1], c=\"red\", alpha=0.5, label=\"Class 1\")\nplt.legend()\nplt.title(\"Exercise 1 \u2014 Generated Data (Class 0 vs Class 1)\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>Figure 1 \u2014 Scatter plot of the two generated classes. Class 0 (blue) is centered at (1.5, 1.5), while Class 1 (red) is centered at (5, 5). The two distributions are clearly separated, confirming linear separability.</p>"},{"location":"exercicios/ex2-perceptron/main/#3-perceptron-implementation","title":"3) Perceptron Implementation","text":""},{"location":"exercicios/ex2-perceptron/main/#a-model-parameters","title":"a. Model Parameters","text":"<p>At its core, the perceptron keeps two types of parameters:</p> <ul> <li>Weights (<code>w</code>) \u2192 control the importance of each input feature.  </li> <li>Bias (<code>b</code>) \u2192 shifts the decision boundary away from the origin.  </li> </ul> <p>Together, they form a linear equation:</p> \\[ z = w_1 \\cdot x_1 + w_2 \\cdot x_2 + b \\] <p>If \\(z \\geq 0\\), the perceptron predicts Class 1. If \\(z &lt; 0\\), it predicts Class 0.</p> Code \u2014 Initialization <pre><code>class Perceptron:\n    def __init__(self, input_dim, learning_rate=0.01, max_epochs=100):\n        self.w = np.zeros(input_dim)   # weights\n        self.b = 0.0                   # bias\n        self.lr = learning_rate\n        self.max_epochs = max_epochs\n        self.accuracy_history = []\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#b-activation-function","title":"b. Activation Function","text":"<p>The perceptron uses a step function as its activation:  </p> <ul> <li>If the input is positive \u2192 output = 1.  </li> <li>Otherwise \u2192 output = 0.  </li> </ul> Code \u2014 Activation <pre><code>    def activation(self, z):\n        return 1 if z &gt;= 0 else 0\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#c-making-predictions","title":"c. Making Predictions","text":"<p>To classify a new point, the perceptron first computes the weighted sum of the inputs plus the bias:</p> \\[ z = w_1 \\cdot x_1 + w_2 \\cdot x_2 + b \\] <p>Then, it applies the step activation function:</p> \\[ \\hat{y} = \\begin{cases} 1, &amp; \\text{if } z \\geq 0 \\\\ 0, &amp; \\text{if } z &lt; 0 \\end{cases} \\] Code \u2014 Prediction <pre><code>    def predict(self, X):\n        z = np.dot(X, self.w) + self.b\n        return np.where(z &gt;= 0, 1, 0)\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#d-learning-rule-training","title":"d. Learning Rule (Training)","text":"<p>The perceptron learns by trial and error:</p> <ol> <li>Start with weights = 0 and bias = 0.  </li> <li>For each training example:<ul> <li>Compute the prediction.  </li> <li>Compare it to the true label.  </li> <li>If wrong, nudge the line by updating weights and bias.  </li> </ul> </li> </ol> <p>Update rule:</p> \\[ w_i \\leftarrow w_i + \\eta \\cdot (y - \\hat{y}) \\cdot x_i \\quad  \\] \\[ b \\leftarrow b + \\eta \\cdot (y - \\hat{y}) \\] <p>Where:</p> <ul> <li>\\(y\\) = true label  </li> <li>\\(\\hat{y}\\) = predicted label  </li> <li>\\(\\eta\\) = learning rate  </li> </ul> <p>Over time, the line moves so that it better separates the classes.</p> Code \u2014 Training <pre><code>    def fit(self, X, y):\n        for epoch in range(self.max_epochs):\n            errors = 0\n            for xi, target in zip(X, y):\n                prediction = self.activation(np.dot(xi, self.w) + self.b)\n                error = target - prediction\n                if error != 0:  # misclassified\n                    self.w += self.lr * error * xi\n                    self.b += self.lr * error\n                    errors += 1\n\n            # Track accuracy\n            predictions = self.predict(X)\n            accuracy = np.mean(predictions == y)\n            self.accuracy_history.append(accuracy)\n\n            if errors == 0:\n                print(f\"Converged after {epoch+1} epochs.\")\n                break\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#e-evaluating-the-model","title":"e. Evaluating the Model","text":"<p>Once trained, the perceptron can be evaluated by checking how many points it classifies correctly (accuracy).</p> Code \u2014 Score Function <pre><code>    def score(self, X, y):\n        predictions = self.predict(X)\n        return np.mean(predictions == y)\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#f-training-on-our-data","title":"f. Training on Our Data","text":"<p>We now train the perceptron on the dataset generated earlier. Since the data is linearly separable, we expect the perceptron to converge quickly and reach 100% accuracy.</p> Code \u2014 Running the Model <pre><code>perceptron = Perceptron(input_dim=2, learning_rate=0.01, max_epochs=100)\nperceptron.fit(X, y)\n\nfinal_accuracy = perceptron.score(X, y)\nprint(\"Final Weights:\", perceptron.w)\nprint(\"Final Bias:\", perceptron.b)\nprint(\"Final Accuracy:\", final_accuracy)\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#4-results-and-conclusions","title":"4) Results and Conclusions","text":"<p>After training the perceptron on the linearly separable dataset, we obtained the following results:</p> <ul> <li>Converged after 10 epochs </li> <li>Final Weights: \\([0.0236, \\; 0.0219]\\) </li> <li>Final Bias: \\(-0.16\\) </li> <li>Final Accuracy: 100%  </li> </ul>"},{"location":"exercicios/ex2-perceptron/main/#a-decision-boundary","title":"a. Decision Boundary","text":"<p>The decision boundary is defined by the linear equation:</p> \\[ w_1 \\cdot x_1 + w_2 \\cdot x_2 + b = 0 \\] <p>Substituting the final parameters:</p> \\[ 0.0236 \\cdot x_1 + 0.0219 \\cdot x_2 - 0.16 = 0 \\] <p>This line divides the 2D space into two regions:</p> <ul> <li>Points classified as Class 0 (blue) when \\(z &lt; 0\\).  </li> <li>Points classified as Class 1 (red) when \\(z \\geq 0\\).  </li> </ul> Code \u2014 Plot Decision Boundary <pre><code>def plot_decision_boundary(X, y, model):\n  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n  xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                      np.linspace(y_min, y_max, 200))\n  grid = np.c_[xx.ravel(), yy.ravel()]\n  Z = model.predict(grid).reshape(xx.shape)\n\n  # Use bwr colormap so 0=blue, 1=red\n  plt.contourf(xx, yy, Z, alpha=0.2, cmap=plt.cm.bwr)\n\n  plt.scatter(X[y==0][:, 0], X[y==0][:, 1], c=\"blue\", label=\"Class 0\", alpha=0.5)\n  plt.scatter(X[y==1][:, 0], X[y==1][:, 1], c=\"red\", label=\"Class 1\", alpha=0.5)\n\n  # Highlight misclassified points\n  predictions = model.predict(X)\n  misclassified = X[predictions != y]\n  if len(misclassified) &gt; 0:\n      plt.scatter(misclassified[:, 0], misclassified[:, 1],\n                  c=\"black\", marker=\"x\", label=\"Misclassified\")\n\n  plt.legend()\n  plt.title(\"Decision Boundary with Data Points\")\n  plt.xlabel(\"x1\")\n  plt.ylabel(\"x2\")\n  plt.show()\n</code></pre> <p></p> <p>Figure 2 \u2014 Final decision boundary learned by the perceptron. The separation line clearly divides Class 0 (blue) and Class 1 (red).</p>"},{"location":"exercicios/ex2-perceptron/main/#b-accuracy-over-epochs","title":"b. Accuracy over Epochs","text":"<p>During training, the perceptron progressively adjusted the weights and bias, improving classification accuracy at each epoch. Convergence was reached at epoch 10, after which no further updates were necessary.</p> Code \u2014 Accuracy Plot <pre><code>plt.plot(range(1, len(perceptron.accuracy_history)+1),\n         perceptron.accuracy_history, marker=\"o\")\nplt.title(\"Training Accuracy over Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n</code></pre> <p></p> <p>Figure 3 \u2014 Accuracy progression over epochs. The perceptron starts near random guessing (~50%) and quickly converges to 100% accuracy by epoch 10.</p>"},{"location":"exercicios/ex2-perceptron/main/#c-discussion","title":"c. Discussion","text":"<p>The results highlight the fundamental properties of the perceptron:</p> <p>The perceptron converged after only 10 epochs, reaching 100% accuracy with final weights  [ 0.0236 , 0.0219 ] [0.0236,0.0219] and bias  \u2212 0.16 \u22120.16. This quick convergence happens because the dataset is linearly separable: a straight line is sufficient to perfectly divide the two classes. In such cases, the perceptron learning rule guarantees convergence in a finite number of steps, and the updates quickly adjust the decision boundary to separate the clusters without errors.</p>"},{"location":"exercicios/ex2-perceptron/main/#exercise-2","title":"Exercise 2","text":""},{"location":"exercicios/ex2-perceptron/main/#1-data-generation_1","title":"1) Data Generation","text":"<p>We generate two classes of 2D data points (1000 samples each) using multivariate normal distributions.  </p> <ul> <li> <p>Class 0:   Mean = [3, 3]   Covariance matrix = [[1.5, 0], [0, 1.5]]</p> </li> <li> <p>Class 1:   Mean = [4, 4]   Covariance matrix = [[1.5, 0], [0, 1.5]]</p> </li> </ul> Code \u2014 Data Generation <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nmean_class0 = [3, 3]\ncov_class0 = [[1.5, 0], [0, 1.5]]\n\nmean_class1 = [4, 4]\ncov_class1 = [[1.5, 0], [0, 1.5]]\n\nn_samples = 1000\nclass0 = np.random.multivariate_normal(mean_class0, cov_class0, n_samples)\nclass1 = np.random.multivariate_normal(mean_class1, cov_class1, n_samples)\n\nX2 = np.vstack((class0, class1))\ny2 = np.hstack((np.zeros(n_samples), np.ones(n_samples)))\n\nplt.figure(figsize=(7, 7))\nplt.scatter(class0[:, 0], class0[:, 1], c=\"blue\", alpha=0.5, label=\"Class 0\")\nplt.scatter(class1[:, 0], class1[:, 1], c=\"red\", alpha=0.5, label=\"Class 1\")\nplt.legend()\nplt.title(\"Exercise 2 - Generated Data (Class 0 vs Class 1)\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>Figure 1 \u2014 Generated dataset. Strong overlap exists between Class 0 (blue) and Class 1 (red).</p>"},{"location":"exercicios/ex2-perceptron/main/#2-perceptron-training","title":"2) Perceptron Training","text":"<p>We reuse the same perceptron implementation from Exercise 1.  </p> Code \u2014 Training <pre><code>perceptron2 = Perceptron(input_dim=2, learning_rate=0.01, max_epochs=100)\nperceptron2.fit(X2, y2)\n\nfinal_accuracy2 = perceptron2.score(X2, y2)\nprint(\"Final Weights:\", perceptron2.w)\nprint(\"Final Bias:\", perceptron2.b)\nprint(\"Final Accuracy:\", final_accuracy2)\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#3-results","title":"3) Results","text":"<p>We visualize the decision boundary and training accuracy.</p> Code \u2014 Decision Boundary <pre><code>plot_decision_boundary(X2, y2, perceptron2)\n</code></pre> <p> Figure 2 \u2014 Decision boundary found by the perceptron. Misclassified points are visible due to overlap.</p> Code \u2014 Accuracy over Epochs <pre><code>plt.plot(range(1, len(perceptron2.accuracy_history)+1),\n         perceptron2.accuracy_history, marker=\"o\")\nplt.title(\"Training Accuracy over Epochs (Exercise 2)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.grid(True)\nplt.show()\n</code></pre> <p> Figure 3 \u2014 Accuracy progression over 100 epochs. The model oscillates around ~50%, never reaching full convergence.</p>"},{"location":"exercicios/ex2-perceptron/main/#4-conclusion","title":"4) Conclusion","text":"<p>The perceptron was trained on a dataset where the two classes present significant overlap. As a result, the algorithm was unable to find a linear decision boundary capable of perfectly separating the points. The decision boundary obtained (Figure 2) shows that many samples from both classes remain in the wrong region, confirming that misclassifications are unavoidable in this scenario. This behavior is also reflected in the training accuracy curve (Figure 3), which fluctuates around 50% and does not stabilize, indicating that the model could not effectively learn a separation rule. Since no epoch achieved a state without errors, the algorithm did not converge at any point during training, reaching the maximum of 100 epochs without success. These results reinforce one of the fundamental limitations of the perceptron: it only converges when the data is linearly separable. In problems where the distributions of the classes overlap, as in this exercise, the perceptron oscillates indefinitely and its accuracy remains close to random guessing. </p>"},{"location":"exercicios/ex3-mlp/ex1/main/","title":"Exercise 1","text":""},{"location":"exercicios/ex3-mlp/ex1/main/#exercise-1-manual-calculation-of-mlp-steps","title":"Exercise 1 \u2014 Manual Calculation of MLP Steps","text":"<p>Warning</p> <p>If the mathematical equations are not displaying correctly, try refreshing the page to fix the rendering.</p>"},{"location":"exercicios/ex3-mlp/ex1/main/#1-initial-setup","title":"1) Initial Setup","text":"<p>We start by defining the input vector, target output, weights, biases, and learning rate as given in the exercise statement.</p> <ul> <li> <p>Input vector: $$ x = [0.5, -0.2] $$</p> </li> <li> <p>Target output: $$ y = 1.0 $$</p> </li> <li> <p>Hidden layer weights:</p> </li> </ul> \\[ W^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} \\] <ul> <li> <p>Hidden layer biases: $$ b^{(1)} = [0.1, -0.2] $$</p> </li> <li> <p>Output layer weights: $$ W^{(2)} = [0.5, -0.3] $$</p> </li> <li> <p>Output layer bias: $$ b^{(2)} = 0.2 $$</p> </li> <li> <p>Learning rate: $$ \\eta = 0.3 $$</p> </li> </ul> Code \u2014 Initial Setup <pre><code>import numpy as np\n\n# Input vector and target output\nx = np.array([0.5, -0.2])\ny = np.array([1.0])\n\n# Hidden layer weights\nW1 = np.array([[0.3, -0.1],\n               [0.2,  0.4]])\n\n# Hidden layer biases\nb1 = np.array([0.1, -0.2])\n\n# Output layer weights\nW2 = np.array([0.5, -0.3])\n\n# Output layer bias\nb2 = np.array([0.2])\n\n# Learning rate\neta = 0.3\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex1/main/#2-forward-pass","title":"2) Forward Pass","text":"<p>Compute the hidden layer pre-activations:</p> \\[ z^{(1)} = W^{(1)}x + b^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ -0.2 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix} \\] <p>Result:</p> \\[ z^{(1)} = [0.27, -0.18] \\] <p>Apply tanh to get hidden activations:</p> \\[ h^{(1)} = \\tanh(z^{(1)}) = [\\tanh(0.27), \\tanh(-0.18)] \\] <p>Result:</p> \\[ h^{(1)} \\approx [0.2636, -0.1780] \\] <p>Compute the output pre-activation:</p> \\[ u^{(2)} = W^{(2)}h^{(1)} + b^{(2)} = [0.5, -0.3] \\begin{bmatrix} 0.2636 \\\\ -0.1780 \\end{bmatrix} + 0.2 \\] <p>Result:</p> \\[ u^{(2)} \\approx 0.3853 \\] <p>Compute the final output:</p> \\[ \\hat{y} = \\tanh(u^{(2)}) = \\tanh(0.3853) \\] <p>Result:</p> \\[ \\hat{y} \\approx 0.3672 \\] Code \u2014 Forward Pass <pre><code># Hidden layer pre-activation\nz1_1 = W1[0,0]*x[0] + W1[0,1]*x[1] + b1[0]\nz1_2 = W1[1,0]*x[0] + W1[1,1]*x[1] + b1[1]\nz1 = [z1_1, z1_2]\n\n# Hidden layer activation\nh1_1 = np.tanh(z1_1)\nh1_2 = np.tanh(z1_2)\nh1 = [h1_1, h1_2]\n\n# Output layer pre-activation\nu2 = W2[0]*h1_1 + W2[1]*h1_2 + b2[0]\n\n# Final output\ny_hat = np.tanh(u2)\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex1/main/#3-loss-calculation","title":"3) Loss Calculation","text":"<p>We compute the error using the Mean Squared Error (MSE):</p> \\[ L = \\frac{1}{N}(y_i - \\hat{y}_i)^2 \\] <p>Since we only have one sample (\\(N=1\\)):</p> \\[ L = (y - \\hat{y})^2 = (1.0 - 0.3672)^2 \\] <p>Result:</p> \\[ L \\approx 0.4004 \\] Code \u2014 Loss Calculation <pre><code># Number of samples (N = 1 here)\nN = 1\n\n# Compute loss\nL = (1/N) * ((y[0] - y_hat)**2)\n\n# Print result\nprint(\"Loss (MSE):\", L)\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex1/main/#4-backward-pass","title":"4) Backward Pass","text":"<p>We now compute the gradients of the loss with respect to all weights and biases.</p> <p>Gradient with respect to the output:</p> \\[ \\frac{\\partial L}{\\partial \\hat{y}} = 2(\\hat{y} - y) \\] <p>Result:</p> \\[ \\frac{\\partial L}{\\partial \\hat{y}} \\approx -1.2656 \\] <p>Gradient with respect to output pre-activation:</p> <p>Since \\( \\hat{y} = \\tanh(u^{(2)}) \\), we use the derivative of tanh:</p> \\[ \\frac{d}{du}\\tanh(u) = 1 - \\tanh^2(u) \\] <p>So:</p> \\[ \\frac{\\partial L}{\\partial u^{(2)}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot (1 - \\tanh^2(u^{(2)})) \\] <p>Result:</p> \\[ \\frac{\\partial L}{\\partial u^{(2)}} \\approx -1.0948 \\] <p>Gradients for the output layer:</p> \\[ \\frac{\\partial L}{\\partial W^{(2)}} = \\frac{\\partial L}{\\partial u^{(2)}} \\cdot h^{(1)}, \\quad  \\frac{\\partial L}{\\partial b^{(2)}} = \\frac{\\partial L}{\\partial u^{(2)}} \\] <p>Results:</p> \\[ \\frac{\\partial L}{\\partial W^{(2)}} \\approx [-0.2886, 0.1950], \\quad \\frac{\\partial L}{\\partial b^{(2)}} \\approx -1.0948 \\] <p>Propagate to the hidden layer:</p> \\[ \\frac{\\partial L}{\\partial h^{(1)}} = \\frac{\\partial L}{\\partial u^{(2)}} \\cdot W^{(2)} \\] <p>Result:</p> \\[ \\frac{\\partial L}{\\partial h^{(1)}} \\approx [-0.5474, 0.3284] \\] <p>Gradient with respect to hidden layer pre-activations:</p> \\[ \\frac{\\partial L}{\\partial z^{(1)}} = \\frac{\\partial L}{\\partial h^{(1)}} \\cdot (1 - \\tanh^2(z^{(1)})) \\] <p>Results:</p> \\[ \\frac{\\partial L}{\\partial z^{(1)}} \\approx [-0.5094, 0.3180] \\] <p>Gradients for hidden layer:</p> \\[ \\frac{\\partial L}{\\partial W^{(1)}} = \\frac{\\partial L}{\\partial z^{(1)}} \\cdot x, \\quad  \\frac{\\partial L}{\\partial b^{(1)}} = \\frac{\\partial L}{\\partial z^{(1)}} \\] <p>Results:</p> \\[ \\frac{\\partial L}{\\partial W^{(1)}} \\approx \\begin{bmatrix} -0.2547 &amp; 0.1019 \\\\ 0.1590 &amp; -0.0636 \\end{bmatrix}, \\quad \\frac{\\partial L}{\\partial b^{(1)}} \\approx [-0.5094, 0.3180] \\] Code \u2014 Backward Pass <pre><code># dL/dy_hat\ndL_dy_hat = 2 * (y_hat - y[0])\n\n# dL/du2 (output pre-activation)\ndL_du2 = dL_dy_hat * (1 - np.tanh(u2)**2)\n\n# Gradients for output layer\ndL_dW2 = [dL_du2 * h1[0], dL_du2 * h1[1]]\ndL_db2 = dL_du2\n\n# Propagate to hidden layer\ndL_dh1 = [dL_du2 * W2[0], dL_du2 * W2[1]]\n\n# dL/dz1\ndL_dz1_1 = dL_dh1[0] * (1 - np.tanh(z1[0])**2)\ndL_dz1_2 = dL_dh1[1] * (1 - np.tanh(z1[1])**2)\ndL_dz1 = [dL_dz1_1, dL_dz1_2]\n\n# Gradients for hidden layer\ndL_dW1 = [[dL_dz1_1 * x[0], dL_dz1_1 * x[1]],\n          [dL_dz1_2 * x[0], dL_dz1_2 * x[1]]]\ndL_db1 = dL_dz1\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex1/main/#5-parameter-update","title":"5) Parameter Update","text":"<p>Using gradient descent, each weight and bias is updated as follows:</p> <ul> <li> <p>Output layer weights: $$ W^{(2)} \\leftarrow W^{(2)} - \\eta \\cdot \\frac{\\partial L}{\\partial W^{(2)}} $$</p> </li> <li> <p>Output layer bias: $$ b^{(2)} \\leftarrow b^{(2)} - \\eta \\cdot \\frac{\\partial L}{\\partial b^{(2)}} $$</p> </li> <li> <p>Hidden layer weights: $$ W^{(1)} \\leftarrow W^{(1)} - \\eta \\cdot \\frac{\\partial L}{\\partial W^{(1)}} $$</p> </li> <li> <p>Hidden layer biases: $$ b^{(1)} \\leftarrow b^{(1)} - \\eta \\cdot \\frac{\\partial L}{\\partial b^{(1)}} $$</p> </li> </ul>"},{"location":"exercicios/ex3-mlp/ex1/main/#numerical-results-03","title":"Numerical Results (\u03b7 = 0.3)","text":"<ul> <li> <p>Updated output weights: $$ W^{(2)} \\approx [0.5866, -0.3585] $$</p> </li> <li> <p>Updated output bias: $$ b^{(2)} \\approx 0.5284 $$</p> </li> <li> <p>Updated hidden weights: $$ W^{(1)} \\approx \\begin{bmatrix} 0.3764 &amp; -0.1306 \\ 0.1523 &amp; 0.4191 \\end{bmatrix} $$</p> </li> <li> <p>Updated hidden biases: $$ b^{(1)} \\approx [0.2528, -0.2954] $$</p> </li> </ul> Code \u2014 Parameter Update <pre><code># Update output layer\nW2_new = [W2[0] - eta * dL_dW2[0],\n          W2[1] - eta * dL_dW2[1]]\nb2_new = b2[0] - eta * dL_db2\n\n# Update hidden layer\nW1_new = [[W1[0,0] - eta * dL_dW1[0][0], W1[0,1] - eta * dL_dW1[0][1]],\n          [W1[1,0] - eta * dL_dW1[1][0], W1[1,1] - eta * dL_dW1[1][1]]]\nb1_new = [b1[0] - eta * dL_db1[0],\n          b1[1] - eta * dL_db1[1]]\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex2/main/","title":"Exercise 2","text":""},{"location":"exercicios/ex3-mlp/ex2/main/#exercise-2-binary-classification-with-synthetic-data-and-scratch-mlp","title":"Exercise 2 \u2014 Binary Classification with Synthetic Data and Scratch MLP","text":"<p>Warning</p> <p>If the mathematical equations are not displaying correctly, try refreshing the page to fix the rendering.</p>"},{"location":"exercicios/ex3-mlp/ex2/main/#1-data-generation","title":"1) Data Generation","text":"<p>We generate a binary dataset with 1000 samples in total: - Class 0 \u2192 generated with 1 cluster (500 samples). - Class 1 \u2192 generated with 2 clusters (500 samples). - Each point has 2 features. - Data is split into 80% training and 20% testing.  </p> <p>This setup ensures the two classes are not linearly separable, making the problem more challenging.</p> Code \u2014 Data Generation <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# Class 0: 1 cluster\nX0, y0 = make_classification(\n    n_samples=500, n_features=2,\n    n_informative=2, n_redundant=0,\n    n_clusters_per_class=1, n_classes=1,\n    class_sep=2.0, random_state=42\n)\ny0 = np.zeros(len(y0))\n\n# Class 1: 2 clusters\nX1, y1 = make_classification(\n    n_samples=500, n_features=2,\n    n_informative=2, n_redundant=0,\n    n_clusters_per_class=2, n_classes=1,\n    class_sep=2.0, random_state=43\n)\ny1 = np.ones(len(y1))\n\n# Combine datasets\nX = np.vstack([X0, X1])\ny = np.hstack([y0, y1])\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Visualization\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1],\n            c='red', alpha=0.6, label='Class 0 (1 cluster)')\nplt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1],\n            c='blue', alpha=0.6, label='Class 1 (2 clusters)')\nplt.title('Training Data Distribution')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_test[y_test==0, 0], X_test[y_test==0, 1],\n            c='red', alpha=0.6, label='Class 0')\nplt.scatter(X_test[y_test==1, 0], X_test[y_test==1, 1],\n            c='blue', alpha=0.6, label='Class 1')\nplt.title('Test Data Distribution')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex2/main/#dataset-visualization","title":"Dataset Visualization","text":"<p>Figure \u2014 Training (left) and test (right) distributions of Class 0 (red) and Class 1 (blue). Class 0 is represented by a single cluster, while Class 1 is represented by two clusters.</p>"},{"location":"exercicios/ex3-mlp/ex2/main/#2-mlp-implementation","title":"2) MLP Implementation","text":"<p>We now implement a Multi-Layer Perceptron (MLP) from scratch using only NumPy. The model contains: - Input layer: number of features = 2 - Hidden layer: 8 neurons with sigmoid activation - Output layer: 1 neuron with sigmoid activation (binary classification) - Loss: Binary Cross-Entropy (BCE) - Optimizer: Gradient Descent  </p>"},{"location":"exercicios/ex3-mlp/ex2/main/#a-initialization","title":"a) Initialization","text":"<p>The weights are initialized using Xavier initialization for faster convergence. Biases start at zero.</p> Code \u2014 Initialization <pre><code>class SimpleMLP:\n    def __init__(self, input_dim, hidden_dim=8, output_dim=1, learning_rate=0.1):\n        # Xavier initialization\n        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)\n        self.b1 = np.zeros((1, hidden_dim))\n        self.W2 = np.random.randn(hidden_dim, output_dim) * np.sqrt(2.0 / hidden_dim)\n        self.b2 = np.zeros((1, output_dim))\n        self.lr = learning_rate\n\n        # Placeholders for intermediate values\n        self.X, self.Z1, self.A1, self.Z2, self.A2 = None, None, None, None, None\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex2/main/#b-forward-pass","title":"b) Forward Pass","text":"<p>The forward pass computes the activations of the hidden and output layers.</p> <ul> <li> <p>Hidden layer:   $$   Z^{(1)} = XW^{(1)} + b^{(1)}, \\quad A^{(1)} = \\sigma(Z^{(1)})   $$</p> </li> <li> <p>Output layer:   $$   Z^{(2)} = A^{(1)} W^{(2)} + b^{(2)}, \\quad A^{(2)} = \\sigma(Z^{(2)})   $$</p> </li> </ul> Code \u2014 Forward Pass <pre><code>    def forward(self, X):\n        self.X = X\n        # Hidden layer\n        self.Z1 = np.dot(X, self.W1) + self.b1\n        self.A1 = sigmoid(self.Z1)\n        # Output layer\n        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n        self.A2 = sigmoid(self.Z2)\n        return self.A2\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex2/main/#c-backward-pass","title":"c) Backward Pass","text":"<p>Gradients are computed using backpropagation:</p> <ul> <li> <p>Output layer error:   $$   dZ^{(2)} = A^{(2)} - y   $$</p> </li> <li> <p>Hidden layer error:   $$   dZ^{(1)} = (dZ^{(2)} W^{(2)T}) \\cdot (A^{(1)} (1-A^{(1)}))   $$</p> </li> </ul> <p>The weights and biases are updated with gradient descent.</p> Code \u2014 Backward Pass <pre><code>    def backward(self, y):\n        m = self.X.shape[0]\n        y = y.reshape(-1, 1)\n\n        # Output gradients\n        dZ2 = self.A2 - y\n        dW2 = (1/m) * np.dot(self.A1.T, dZ2)\n        db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n\n        # Hidden gradients\n        dA1 = np.dot(dZ2, self.W2.T)\n        dZ1 = dA1 * self.A1 * (1 - self.A1)\n        dW1 = (1/m) * np.dot(self.X.T, dZ1)\n        db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n\n        # Parameter update\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex2/main/#d-training-fit-method","title":"d) Training (Fit Method)","text":"<p>The training loop repeats: 1. Forward pass 2. Loss calculation 3. Backward pass (update parameters)  </p> Code \u2014 Fit Method <pre><code>    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat = self.forward(X)\n            loss = binary_cross_entropy(y, y_hat.flatten())\n            losses.append(loss)\n            self.backward(y)\n\n            if (epoch+1) % 100 == 0:\n                acc = accuracy_score(y, self.predict(X))\n                print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Acc: {acc:.4f}\")\n        return losses\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex2/main/#e-predictions","title":"e) Predictions","text":"<p>The network can output: - Predicted probabilities - Binary predictions with a decision threshold  </p> Code \u2014 Prediction <pre><code>    def predict(self, X, threshold=0.5):\n        probs = self.forward(X)\n        return (probs.flatten() &gt;= threshold).astype(int)\n\n    def predict_proba(self, X):\n        return self.forward(X).flatten()\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex2/main/#3-training","title":"3) Training","text":"<p>We now initialize the MLP with: - Input dimension: 2 (two features) - Hidden layer: 8 neurons - Output dimension: 1 neuron (binary classification) - Learning rate: \\( \\eta = 0.3 \\) - Number of epochs: 500  </p> <p>The training process consists of: 1. Forward pass (prediction) 2. Loss calculation (Binary Cross-Entropy) 3. Backward pass (gradient computation and parameter update) 4. Repeat for all epochs  </p> Code \u2014 Training <pre><code>mlp = SimpleMLP(input_dim=2, hidden_dim=8, output_dim=1, learning_rate=0.3)\nlosses = mlp.fit(X_train, y_train, epochs=500)\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex2/main/#4-results-and-visualizations","title":"4) Results and Visualizations","text":""},{"location":"exercicios/ex3-mlp/ex2/main/#a-training-loss-curve","title":"a) Training Loss Curve","text":"<p>The Binary Cross-Entropy (BCE) loss decreases consistently over epochs, starting near 0.83 and stabilizing around 0.44 after 500 epochs. This shows that the model is effectively learning to minimize classification error.</p>"},{"location":"exercicios/ex3-mlp/ex2/main/#b-confusion-matrix","title":"b) Confusion Matrix","text":"<p>The test set evaluation gives an accuracy of approximately 79%.  </p> <ul> <li>Class 0: 96 correctly classified, 4 misclassified.  </li> <li>Class 1: 62 correctly classified, 38 misclassified.  </li> </ul> <p>This indicates the model performs better on Class 0 (1 cluster) than on Class 1 (2 clusters), which is expected since Class 1 is harder to separate due to its two-cluster distribution.</p> <p></p> <p>Figure \u2014 Training loss curve (BCE) showing progressive improvement of the model and Confusion Matrix for the test set. Model accuracy \u2248 79%.</p>"},{"location":"exercicios/ex3-mlp/ex2/main/#c-decision-boundary-training-data","title":"c) Decision Boundary (Training Data)","text":"<p>The decision boundary learned by the MLP is non-linear and adapts to the overlap between classes. On the training data, the separation is clear, though misclassified points appear near the boundary where clusters overlap.</p> <p></p> <p>Figure \u2014 Decision boundary visualization on training data. The boundary curves to adapt to the two clusters of Class 1.</p>"},{"location":"exercicios/ex3-mlp/ex2/main/#d-decision-boundary-test-data","title":"d) Decision Boundary (Test Data)","text":"<p>On the test set, the decision boundary generalizes well but some misclassification persists in regions where Class 0 and Class 1 overlap. This explains the lower accuracy compared to training, but it confirms that the MLP is capturing the underlying structure rather than simply memorizing.</p> <p></p> <p>Figure \u2014 Decision boundary visualization on test data. Misclassifications occur in overlapping regions.</p>"},{"location":"exercicios/ex3-mlp/ex3/main/","title":"Exercise 3","text":""},{"location":"exercicios/ex3-mlp/ex3/main/#exercise-3-multi-class-classification-with-synthetic-data-and-reusable-mlp","title":"Exercise 3 \u2014 Multi-Class Classification with Synthetic Data and Reusable MLP","text":""},{"location":"exercicios/ex3-mlp/ex3/main/#1-data-generation","title":"1) Data Generation","text":"<p>We create a dataset with 3 classes and 1500 samples, each with 4 features: - Class 0 \u2192 2 clusters - Class 1 \u2192 3 clusters - Class 2 \u2192 4 clusters  </p> <p>Each class has 500 samples, and the dataset is split into 80% training (1200 samples) and 20% testing (300 samples).</p> Code \u2014 Data Generation <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\n\ndef create_multiclass_dataset():\n    # Class 0: 2 clusters\n    X0, _ = make_classification(\n        n_samples=500, n_features=4,\n        n_informative=4, n_redundant=0,\n        n_clusters_per_class=2, n_classes=1,\n        class_sep=2.0, random_state=42\n    )\n    y0 = np.zeros(len(X0))\n\n    # Class 1: 3 clusters\n    X1, _ = make_classification(\n        n_samples=500, n_features=4,\n        n_informative=4, n_redundant=0,\n        n_clusters_per_class=3, n_classes=1,\n        class_sep=2.0, random_state=43\n    )\n    y1 = np.ones(len(X1))\n\n    # Class 2: 4 clusters\n    X2, _ = make_classification(\n        n_samples=500, n_features=4,\n        n_informative=4, n_redundant=0,\n        n_clusters_per_class=4, n_classes=1,\n        class_sep=2.0, random_state=44\n    )\n    y2 = np.full(len(X2), 2)\n\n    # Combine and shuffle\n    X = np.vstack([X0, X1, X2])\n    y = np.hstack([y0, y1, y2])\n    indices = np.random.RandomState(42).permutation(len(X))[:1500]\n    return X[indices], y[indices]\n\nX, y = create_multiclass_dataset()\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex3/main/#2-helper-functions","title":"2) Helper Functions","text":"<p>We reuse helper functions from Exercise 2, adding softmax and categorical cross-entropy for multi-class.</p> Code \u2014 Helper Functions <pre><code>def sigmoid(x):\n    x = np.clip(x, -250, 250)  # prevent overflow\n    return 1 / (1 + np.exp(-x))\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n\ndef categorical_cross_entropy(y_true, y_pred):\n    eps = 1e-15\n    y_pred = np.clip(y_pred, eps, 1 - eps)\n    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n\ndef to_categorical(y, num_classes):\n    categorical = np.zeros((len(y), num_classes))\n    categorical[np.arange(len(y)), y.astype(int)] = 1\n    return categorical\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex3/main/#3-mlp-implementation","title":"3) MLP Implementation","text":"<p>The MLP structure is the same as in Exercise 2. The only change: - Output activation \u2192 softmax - Loss function \u2192 categorical cross-entropy </p> Code \u2014 MLP Implementation <pre><code>class MLP:\n    def __init__(self, input_dim, hidden_dim=8, output_dim=3, learning_rate=0.1):\n        # Xavier initialization\n        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)\n        self.b1 = np.zeros((1, hidden_dim))\n        self.W2 = np.random.randn(hidden_dim, output_dim) * np.sqrt(2.0 / hidden_dim)\n        self.b2 = np.zeros((1, output_dim))\n        self.lr = learning_rate\n\n    def forward(self, X):\n        self.X = X\n        self.Z1 = np.dot(X, self.W1) + self.b1\n        self.A1 = sigmoid(self.Z1)\n        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n        self.A2 = softmax(self.Z2)\n        return self.A2\n\n    def backward(self, y):\n        m = self.X.shape[0]\n        dZ2 = self.A2 - y\n        dW2 = (1/m) * np.dot(self.A1.T, dZ2)\n        db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n        dA1 = np.dot(dZ2, self.W2.T)\n        dZ1 = dA1 * self.A1 * (1 - self.A1)\n        dW1 = (1/m) * np.dot(self.X.T, dZ1)\n        db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n\n    def fit(self, X, y, epochs=500):\n        losses = []\n        for epoch in range(epochs):\n            y_hat = self.forward(X)\n            loss = categorical_cross_entropy(y, y_hat)\n            losses.append(loss)\n            self.backward(y)\n        return losses\n\n    def predict(self, X):\n        probs = self.forward(X)\n        return np.argmax(probs, axis=1)\n\n    def predict_proba(self, X):\n        return self.forward(X)\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex3/main/#4-training","title":"4) Training","text":"<p>We convert labels to one-hot encoding and train for 500 epochs.</p> Code \u2014 Training <pre><code>y_train_categorical = to_categorical(y_train, 3)\ny_test_categorical = to_categorical(y_test, 3)\n\nmlp = MLP(input_dim=4, hidden_dim=8, output_dim=3, learning_rate=0.1)\nlosses = mlp.fit(X_train, y_train_categorical, epochs=500)\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex3/main/#5-results-and-visualizations","title":"5) Results and Visualizations","text":"<ul> <li>Training loss curve </li> <li>Confusion matrix on test set </li> <li>Combined in a single figure.  </li> </ul> Code \u2014 Results and Visualizations <pre><code># Training loss + Confusion Matrix\nplt.figure(figsize=(12, 4))\n\n# Loss curve\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(losses)+1), losses, 'b-', linewidth=2)\nplt.title(\"Training Loss (Categorical Cross-Entropy)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Categorical CE Loss\")\nplt.grid(True, alpha=0.3)\n\n# Confusion matrix\ny_pred = mlp.predict(X_test)\ntest_acc = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\nplt.subplot(1, 2, 2)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Class 0', 'Class 1', 'Class 2'],\n            yticklabels=['Class 0', 'Class 1', 'Class 2'])\nplt.title(f'Confusion Matrix\\nAccuracy: {test_acc:.3f}')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Output:</p> <p></p> <p>Figure \u2014 Training loss curve (left) and confusion matrix (right) for the multi-class classification task. Final test accuracy \u2248 0.867.</p>"},{"location":"exercicios/ex3-mlp/ex3/main/#6-conclusion","title":"6) Conclusion","text":"<p>The MLP was successfully extended from the binary case (Exercise 2) to the multi-class case (Exercise 3). Only two modifications were required: using the softmax function at the output and adopting categorical cross-entropy as the loss function. After 500 training epochs, the network achieved a test accuracy of 86.7%. Most errors were concentrated in Class 2, which had more clusters and overlapping regions. These results demonstrate the generalization ability of MLPs in multi-class classification problems.</p>"},{"location":"exercicios/ex3-mlp/ex4/main/","title":"Exercise 4","text":""},{"location":"exercicios/ex3-mlp/ex4/main/#exercise-4-multi-class-classification-with-deeper-mlp","title":"Exercise 4 \u2014 Multi-Class Classification with Deeper MLP","text":""},{"location":"exercicios/ex3-mlp/ex4/main/#1-data-generation","title":"1) Data Generation","text":"<p>We reuse the same dataset structure from previous exercises to keep comparisons fair. - Synthetic dataset with multiple classes - Features: N - Train/test split: 80/20  </p> Code \u2014 Data Generation <pre><code># (reusing the same data generation as in Exercise 3, or with modifications)\nX, y = create_multiclass_dataset()\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex4/main/#2-helper-functions","title":"2) Helper Functions","text":"<p>The same activation functions and loss as in Exercise 3 are used: - Sigmoid for hidden layers - Softmax for output - Categorical Cross-Entropy for loss  </p> Code \u2014 Helper Functions <pre><code>def sigmoid(x):\n    return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n\ndef categorical_cross_entropy(y_true, y_pred):\n    eps = 1e-15\n    y_pred = np.clip(y_pred, eps, 1 - eps)\n    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n\ndef to_categorical(y, num_classes):\n    categorical = np.zeros((len(y), num_classes))\n    categorical[np.arange(len(y)), y.astype(int)] = 1\n    return categorical\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex4/main/#3-mlp-implementation-deeper-network","title":"3) MLP Implementation (Deeper Network)","text":"<p>In this exercise, the MLP is extended with two hidden layers instead of one.</p> <ul> <li>Architecture: Input \u2192 Hidden Layer 1 \u2192 Hidden Layer 2 \u2192 Output  </li> <li>Hidden layers use sigmoid activations  </li> <li>Output layer uses softmax  </li> </ul> Code \u2014 Deeper MLP <pre><code>class DeepMLP:\n    def __init__(self, input_dim, hidden1=16, hidden2=8, output_dim=3, learning_rate=0.1):\n        # Xavier initialization\n        self.W1 = np.random.randn(input_dim, hidden1) * np.sqrt(2.0 / input_dim)\n        self.b1 = np.zeros((1, hidden1))\n        self.W2 = np.random.randn(hidden1, hidden2) * np.sqrt(2.0 / hidden1)\n        self.b2 = np.zeros((1, hidden2))\n        self.W3 = np.random.randn(hidden2, output_dim) * np.sqrt(2.0 / hidden2)\n        self.b3 = np.zeros((1, output_dim))\n        self.lr = learning_rate\n\n    def forward(self, X):\n        self.X = X\n        # Layer 1\n        self.Z1 = np.dot(X, self.W1) + self.b1\n        self.A1 = sigmoid(self.Z1)\n        # Layer 2\n        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n        self.A2 = sigmoid(self.Z2)\n        # Output\n        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n        self.A3 = softmax(self.Z3)\n        return self.A3\n\n    def backward(self, y):\n        m = self.X.shape[0]\n        dZ3 = self.A3 - y\n        dW3 = (1/m) * np.dot(self.A2.T, dZ3)\n        db3 = (1/m) * np.sum(dZ3, axis=0, keepdims=True)\n\n        dA2 = np.dot(dZ3, self.W3.T)\n        dZ2 = dA2 * self.A2 * (1 - self.A2)\n        dW2 = (1/m) * np.dot(self.A1.T, dZ2)\n        db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n\n        dA1 = np.dot(dZ2, self.W2.T)\n        dZ1 = dA1 * self.A1 * (1 - self.A1)\n        dW1 = (1/m) * np.dot(self.X.T, dZ1)\n        db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n\n        # Update\n        self.W3 -= self.lr * dW3\n        self.b3 -= self.lr * db3\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n\n    def fit(self, X, y, epochs=500):\n        losses = []\n        for epoch in range(epochs):\n            y_hat = self.forward(X)\n            loss = categorical_cross_entropy(y, y_hat)\n            losses.append(loss)\n            self.backward(y)\n        return losses\n\n    def predict(self, X):\n        return np.argmax(self.forward(X), axis=1)\n\n    def predict_proba(self, X):\n        return self.forward(X)\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex4/main/#4-training","title":"4) Training","text":"Code \u2014 Training <pre><code>y_train_categorical = to_categorical(y_train, 3)\ny_test_categorical = to_categorical(y_test, 3)\n\nmlp = DeepMLP(input_dim=4, hidden1=16, hidden2=8, output_dim=3, learning_rate=0.1)\nlosses = mlp.fit(X_train, y_train_categorical, epochs=500)\n</code></pre>"},{"location":"exercicios/ex3-mlp/ex4/main/#5-results-and-visualization","title":"5) Results and Visualization","text":"<p>The results are presented as: - Training loss curve (CCE) - Confusion matrix on test set  </p> Code \u2014 Results and Visualization <pre><code>plt.figure(figsize=(12,4))\n\n# Loss curve\nplt.subplot(1,2,1)\nplt.plot(range(1, len(losses)+1), losses, 'b-', linewidth=2)\nplt.title(\"Training Loss (Categorical Cross-Entropy)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"CCE Loss\")\nplt.grid(True, alpha=0.3)\n\n# Confusion Matrix\ny_pred = mlp.predict(X_test)\ntest_acc = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\nplt.subplot(1,2,2)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Class 0','Class 1','Class 2'],\n            yticklabels=['Class 0','Class 1','Class 2'])\nplt.title(f'Confusion Matrix\\nAccuracy: {test_acc:.3f}')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Output:</p> <p></p> <p>Figure \u2014 Training loss curve (left) and confusion matrix (right) for the multi-class classification task.</p>"},{"location":"exercicios/ex3-mlp/ex4/main/#6-conclusion","title":"6) Conclusion","text":"<ul> <li>By adding a second hidden layer, the network gained more capacity to capture complex patterns.  </li> <li>The loss decreased smoothly, and the test set accuracy improved compared to Exercise 3.  </li> <li>However, deeper models require more careful hyperparameter tuning (learning rate, epochs, neurons) to avoid overfitting.  </li> <li>This exercise shows how deeper MLPs scale to more complex tasks by stacking multiple layers.  </li> </ul>"},{"location":"exercicios/ex4-metrics/main/","title":"Main","text":""},{"location":"exercicios/ex4-metrics/main/#exercise-4-deeper-mlp-with-scratch-implementation","title":"Exercise 4 \u2014 Deeper MLP with Scratch Implementation","text":""},{"location":"exercicios/ex4-metrics/main/#1-data-generation","title":"1) Data Generation","text":"<p>We reuse the same dataset structure from previous exercises to keep comparisons fair. - Synthetic dataset with multiple classes - Features: N - Train/test split: 80/20  </p> Code \u2014 Data Generation <pre><code># (reusing the same data generation as in Exercise 3, or with modifications)\nX, y = create_multiclass_dataset()\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n</code></pre>"},{"location":"exercicios/ex4-metrics/main/#2-helper-functions","title":"2) Helper Functions","text":"<p>The same activation functions and loss as in Exercise 3 are used: - Sigmoid for hidden layers - Softmax for output - Categorical Cross-Entropy for loss  </p> Code \u2014 Helper Functions <pre><code>def sigmoid(x):\n    return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n\ndef categorical_cross_entropy(y_true, y_pred):\n    eps = 1e-15\n    y_pred = np.clip(y_pred, eps, 1 - eps)\n    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n\ndef to_categorical(y, num_classes):\n    categorical = np.zeros((len(y), num_classes))\n    categorical[np.arange(len(y)), y.astype(int)] = 1\n    return categorical\n</code></pre>"},{"location":"exercicios/ex4-metrics/main/#3-mlp-implementation-deeper-network","title":"3) MLP Implementation (Deeper Network)","text":"<p>In this exercise, the MLP is extended with two hidden layers instead of one.</p> <ul> <li>Architecture: Input \u2192 Hidden Layer 1 \u2192 Hidden Layer 2 \u2192 Output  </li> <li>Hidden layers use sigmoid activations  </li> <li>Output layer uses softmax  </li> </ul> Code \u2014 Deeper MLP <pre><code>class DeepMLP:\n    def __init__(self, input_dim, hidden1=16, hidden2=8, output_dim=3, learning_rate=0.1):\n        # Xavier initialization\n        self.W1 = np.random.randn(input_dim, hidden1) * np.sqrt(2.0 / input_dim)\n        self.b1 = np.zeros((1, hidden1))\n        self.W2 = np.random.randn(hidden1, hidden2) * np.sqrt(2.0 / hidden1)\n        self.b2 = np.zeros((1, hidden2))\n        self.W3 = np.random.randn(hidden2, output_dim) * np.sqrt(2.0 / hidden2)\n        self.b3 = np.zeros((1, output_dim))\n        self.lr = learning_rate\n\n    def forward(self, X):\n        self.X = X\n        # Layer 1\n        self.Z1 = np.dot(X, self.W1) + self.b1\n        self.A1 = sigmoid(self.Z1)\n        # Layer 2\n        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n        self.A2 = sigmoid(self.Z2)\n        # Output\n        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n        self.A3 = softmax(self.Z3)\n        return self.A3\n\n    def backward(self, y):\n        m = self.X.shape[0]\n        dZ3 = self.A3 - y\n        dW3 = (1/m) * np.dot(self.A2.T, dZ3)\n        db3 = (1/m) * np.sum(dZ3, axis=0, keepdims=True)\n\n        dA2 = np.dot(dZ3, self.W3.T)\n        dZ2 = dA2 * self.A2 * (1 - self.A2)\n        dW2 = (1/m) * np.dot(self.A1.T, dZ2)\n        db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n\n        dA1 = np.dot(dZ2, self.W2.T)\n        dZ1 = dA1 * self.A1 * (1 - self.A1)\n        dW1 = (1/m) * np.dot(self.X.T, dZ1)\n        db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n\n        # Update\n        self.W3 -= self.lr * dW3\n        self.b3 -= self.lr * db3\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n\n    def fit(self, X, y, epochs=500):\n        losses = []\n        for epoch in range(epochs):\n            y_hat = self.forward(X)\n            loss = categorical_cross_entropy(y, y_hat)\n            losses.append(loss)\n            self.backward(y)\n        return losses\n\n    def predict(self, X):\n        return np.argmax(self.forward(X), axis=1)\n\n    def predict_proba(self, X):\n        return self.forward(X)\n</code></pre>"},{"location":"exercicios/ex4-metrics/main/#4-training","title":"4) Training","text":"Code \u2014 Training <pre><code>y_train_categorical = to_categorical(y_train, 3)\ny_test_categorical = to_categorical(y_test, 3)\n\nmlp = DeepMLP(input_dim=4, hidden1=16, hidden2=8, output_dim=3, learning_rate=0.1)\nlosses = mlp.fit(X_train, y_train_categorical, epochs=500)\n</code></pre>"},{"location":"exercicios/ex4-metrics/main/#5-results-and-visualization","title":"5) Results and Visualization","text":"<p>The results are presented as: - Training loss curve (CCE) - Confusion matrix on test set  </p> Code \u2014 Results and Visualization <pre><code>plt.figure(figsize=(12,4))\n\n# Loss curve\nplt.subplot(1,2,1)\nplt.plot(range(1, len(losses)+1), losses, 'b-', linewidth=2)\nplt.title(\"Training Loss (Categorical Cross-Entropy)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"CCE Loss\")\nplt.grid(True, alpha=0.3)\n\n# Confusion Matrix\ny_pred = mlp.predict(X_test)\ntest_acc = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\nplt.subplot(1,2,2)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Class 0','Class 1','Class 2'],\n            yticklabels=['Class 0','Class 1','Class 2'])\nplt.title(f'Confusion Matrix\\nAccuracy: {test_acc:.3f}')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"exercicios/ex4-metrics/main/#6-conclusion","title":"6) Conclusion","text":"<ul> <li>By adding a second hidden layer, the network gained more capacity to capture complex patterns.  </li> <li>The loss decreased smoothly, and the test set accuracy improved compared to Exercise 3.  </li> <li>However, deeper models require more careful hyperparameter tuning (learning rate, epochs, neurons) to avoid overfitting.  </li> <li>This exercise shows how deeper MLPs scale to more complex tasks by stacking multiple layers.  </li> </ul>"},{"location":"projeto/main/","title":"Projects","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"}]}