{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#artificial-neural-networks-portfolio-mateus-marinheiro","title":"Artificial Neural Networks Portfolio - Mateus Marinheiro","text":"<p>Welcome to my portfolio for the Artificial Neural Networks and Deep Learning course. Here I will document all exercises and the final project developed throughout the semester.  </p>"},{"location":"#deliveries","title":"Deliveries","text":"<ul> <li> Exercise 1 \u2014 Data (delivered on 05/09/2025) </li> <li> Exercise 2 \u2014 Perceptron  </li> <li> Exercise 3 \u2014 MLP  </li> <li> Exercise 4 \u2014 Metrics  </li> <li> Final Project </li> </ul>"},{"location":"#organization-of-this-portfolio","title":"Organization of this Portfolio","text":"<ul> <li>Each Exercise has its own page with:</li> <li>Problem statement  </li> <li>Implementation (code snippets)  </li> <li>Results (plots, tables, metrics)  </li> <li> <p>Discussion (analysis and conclusions)  </p> </li> <li> <p>The Final Project will summarize the main learnings and present a complete application of neural networks.</p> </li> </ul>"},{"location":"exercicios/ex1-data/main/","title":"1. Data","text":""},{"location":"exercicios/ex1-data/main/#1-data","title":"1. Data","text":""},{"location":"exercicios/ex1-data/main/#exercise-1-exploring-class-separability-in-2d","title":"Exercise 1 \u2013 Exploring Class Separability in 2D","text":""},{"location":"exercicios/ex1-data/main/#1-data-generation","title":"1) Data Generation","text":"<p>Create a synthetic 2D dataset with 400 samples, evenly split across four classes (100 each). Each class is sampled independently from a Gaussian (normal) distribution with its own mean and per-axis standard deviations:</p> <ul> <li>Class 0: mean \u2248 (2, 3), std \u2248 (0.8, 2.5)</li> <li>Class 1: mean \u2248 (5, 6), std \u2248 (1.2, 1.9)</li> <li>Class 2: mean \u2248 (8, 1), std \u2248 (0.9, 0.9)</li> <li>Class 3: mean \u2248 (15, 4), std \u2248 (0.5, 2.0)</li> </ul> <p>What this means. For each class \\(c\\), we draw 100 points \\((x_1, x_2)\\) where: - \\(x_1 \\sim \\mathcal{N}(\\mu_{c,1}, \\sigma_{c,1})\\) - \\(x_2 \\sim \\mathcal{N}(\\mu_{c,2}, \\sigma_{c,2})\\)</p> <p>This produces four \u201cclouds\u201d of points centered around their means, with vertical/horizontal spread governed by the standard deviations.</p> Data Generation Code <pre><code>import numpy as np\nimport pandas as pd\n\n# Class definitions\nparams = {\n    0: {\"mean\": [2, 3],   \"std\": [0.8, 2.5]},\n    1: {\"mean\": [5, 6],   \"std\": [1.2, 1.9]},\n    2: {\"mean\": [8, 1],   \"std\": [0.9, 0.9]},\n    3: {\"mean\": [15, 4],  \"std\": [0.5, 2.0]},\n}\n\nN = 100\nrng = np.random.default_rng()\n\n# Create individual DataFrames\ndfs = []\nfor cls, p in params.items():\n    x1 = rng.normal(p[\"mean\"][0], p[\"std\"][0], N)\n    x2 = rng.normal(p[\"mean\"][1], p[\"std\"][1], N)\n    df_cls = pd.DataFrame({\"class\": cls, \"x1\": x1, \"x2\": x2})\n    dfs.append(df_cls)\n\n# Concatenate all DataFrames\ndf = pd.concat(dfs, ignore_index=True)\nprint(df.head(10))\n</code></pre>"},{"location":"exercicios/ex1-data/main/#2-plot-the-data","title":"2) Plot the Data","text":"<p>Figure 1 \u2014 2D scatter plot of the generated dataset. Each color represents a different class.</p>"},{"location":"exercicios/ex1-data/main/#3-analyze-and-draw-boundaries","title":"3) Analyze and Draw Boundaries","text":""},{"location":"exercicios/ex1-data/main/#a-scatter-plot-analysis","title":"a. Scatter Plot Analysis","text":"<p>By examining the scatter plot, it is possible to identify the following patterns:</p> <ul> <li>Class 0 (blue) forms a cluster on the left side, centered around (2,3). It shows a high vertical spread, which causes partial overlap with the lower points of Class 1.  </li> <li>Class 1 (orange) is located slightly to the right and above Class 0, centered near (5,6). Due to its vertical dispersion, it overlaps not only with the upper boundary of Class 0 but also approaches the top region of Class 2.  </li> <li>Class 2 (green) is concentrated near (8,1), with a smaller variance, making it more compact. However, it still lies close to the bottom of Class 1, leading to a noticeable boundary interaction.  </li> <li>Class 3 (red) is clearly separated on the far right, around (15,4). This class does not show any significant overlap with the others, making it the easiest to isolate.  </li> </ul> <p>In summary, Classes 0, 1, and 2 show areas of overlap due to their vertical spread and proximity along the x-axis, while Class 3 remains well isolated from the others.</p>"},{"location":"exercicios/ex1-data/main/#b-linear-separability","title":"b. Linear Separability","text":"<p>A single linear boundary cannot separate all four classes. As illustrated by the sketched straight lines (purple for Class 0\u20131, yellow for Class 1\u20132, and a gray vertical line isolating Class 3), we already need multiple linear cuts to approximate the partitions. Even then, misclassifications remain:</p> <ul> <li>Class 3 can be isolated with a vertical line on the far right.</li> <li>Classes 0 and 1 overlap vertically; any straight line will mislabel points near the central band.</li> <li>Classes 1 and 2 intersect around the lower edge of Class 1 and the upper edge of Class 2; a diagonal line reduces but does not eliminate errors.</li> </ul> <p>Therefore, while straight lines help illustrate partial separation, linear decision boundaries alone cannot perfectly separate all classes due to the overlap and shaped spreads of Classes 0, 1, and 2.</p>"},{"location":"exercicios/ex1-data/main/#c-decision-boundaries-sketch","title":"c. Decision boundaries (sketch)","text":"<p>Below I include the sketch of decision boundaries that a trained neural network might learn. In this illustration I use three straight lines: a diagonal separating Classes 0\u20131, another diagonal separating Classes 1\u20132, and a vertical line isolating Class 3 on the far right.</p> <p></p> <p>Figure 2 \u2014 Three straight separators: 0\u20131 (diagonal), 1\u20132 (diagonal), and 2\u20133 (vertical).</p> Code used to draw the sketch (replace coordinates with your final ones) <pre><code>plt.figure(figsize=(10, 6))\n\n# Scatter plot of the classes\nsns.scatterplot(data=df, x=\"x1\", y=\"x2\", hue=\"classe\", palette=\"tab10\")\n\n# Manually sketched straight lines (approximate coordinates)\nline1_x = [2, 5]\nline1_y = [10, -4]\n\nline2_x = [4, 10]\nline2_y = [-0.2, 8]\n\nline3_x = [13, 13]\nline3_y = [-2, 10]\n\n# Plot the lines\nplt.plot(line1_x, line1_y, color=\"purple\", linewidth=2, linestyle=\"--\", label=\"0\u20131 division\")\nplt.plot(line2_x, line2_y, color=\"gold\", linewidth=2, linestyle=\"--\", label=\"1\u20132 division\")\nplt.plot(line3_x, line3_y, color=\"gray\", linewidth=2, linestyle=\"--\", label=\"2\u20133 division\")\n\n# Styling\nplt.title(\"Scatter Plot with Sketched Decision Boundaries\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend(title=\"Class\", loc=\"lower right\")\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"exercicios/ex1-data/main/#exercise-2-non-linearity-in-higher-dimensions","title":"Exercise 2 \u2014 Non-Linearity in Higher Dimensions","text":""},{"location":"exercicios/ex1-data/main/#1-data-generation_1","title":"1) Data Generation","text":"<p>Goal. Create a 5-dimensional dataset with two classes (A and B), each containing 500 samples. The samples are drawn from multivariate Gaussian distributions with different means and covariance matrices:</p> <ul> <li>Class A: centered around the origin (0, 0, 0, 0, 0) with moderate positive correlations among features.  </li> <li>Class B: centered at (1.5, 1.5, 1.5, 1.5, 1.5) with slightly different correlations and variances.  </li> </ul> <p>This setup produces two overlapping but distinguishable high-dimensional clusters, which makes linear separation difficult.</p> Data Generation Code <pre><code>import numpy as np\nimport pandas as pd\n\n# Parameters for Class A\nmu_A = [0, 0, 0, 0, 0]\nSigma_A = [\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0],\n]\n\n# Parameters for Class B\nmu_B = [1.5, 1.5, 1.5, 1.5, 1.5]\nSigma_B = [\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.6, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5],\n]\n\n# Generate 500 samples for each class\nXA = rng.multivariate_normal(mu_A, Sigma_A, size=500)\nXB = rng.multivariate_normal(mu_B, Sigma_B, size=500)\n\n# Merge into a single dataset\nX = np.vstack([XA, XB])\ny = np.array([0]*500 + [1]*500)  # 0 = Class A, 1 = Class B\n\n# Create a DataFrame (Excel-style table)\ncols = [f\"f{i}\" for i in range(1, 6)]\ndf = pd.DataFrame(X, columns=cols)\ndf[\"class\"] = y\n\nprint(df.head())\n</code></pre>"},{"location":"exercicios/ex1-data/main/#2-data-visualization-dimensionality-reduction-with-pca","title":"2) Data Visualization (Dimensionality Reduction with PCA)","text":"<p>Since the dataset lives in a 5-dimensional space, direct visualization is not possible. To explore the separability of the two classes, we apply Principal Component Analysis (PCA) to project the standardized data into two principal components (PC1 and PC2).</p> Code for Dimensionality Reduction and Visualization <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Step 1 \u2014 Standardize features (mean = 0, std = 1 per column)\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n\n# Step 2 \u2014 PCA projection to 2 dimensions\npca = PCA(n_components=2, random_state=42)\nZ = pca.fit_transform(X_std)\n\nprint(\"Explained variance by PC1 and PC2:\",\n      np.round(pca.explained_variance_ratio_, 4))\nprint(\"Total variance explained (PC1+PC2):\",\n      np.round(pca.explained_variance_ratio_.sum(), 4))\n\n# Step 3 \u2014 Scatter plot of projected data\ncolors = {0: \"#1f77b4\", 1: \"#d62728\"}  # Blue = Class A, Red = Class B\nlabels = {0: \"Class A\", 1: \"Class B\"}\n\nplt.figure(figsize=(8, 6))\nfor c in (0, 1):\n    mask = (y == c)\n    plt.scatter(Z[mask, 0], Z[mask, 1],\n                s=18, alpha=0.75,\n                label=labels[c],\n                c=colors[c])\nplt.xlabel(\"Principal Component 1 (PC1)\")\nplt.ylabel(\"Principal Component 2 (PC2)\")\nplt.title(\"PCA Projection of 5D Data onto First Two Principal Components\")\nplt.legend(title=\"Class\")\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Figure 3 \u2014 PCA projection of the 5D dataset into 2D. Each point corresponds to one sample, colored by class (blue = Class A, red = Class B).</p>"},{"location":"exercicios/ex1-data/main/#3-analysis-of-the-pca-projection","title":"3) Analysis of the PCA Projection","text":""},{"location":"exercicios/ex1-data/main/#a-relationship-between-the-two-classes","title":"a. Relationship Between the Two Classes","text":"<p>The scatter plot shows the dataset after projection into the first two principal components (PC1 and PC2). Some clear patterns can be observed:</p> <ul> <li>Class A (blue) is concentrated on the left-hand side of the PC1 axis, although it spreads vertically along PC2.  </li> <li>Class B (red) is mostly located on the right-hand side, also with some vertical dispersion.  </li> <li>The two classes are not completely isolated \u2014 there is a noticeable overlapping region around the center (PC1 \u2248 0), where samples from both classes mix.  </li> </ul> <p>This indicates that while PCA successfully separates the clusters to some extent, the data distributions remain partially entangled.</p>"},{"location":"exercicios/ex1-data/main/#b-linear-separability_1","title":"b. Linear Separability","text":"<p>From the 2D projection, it becomes clear that a single linear boundary (for example, a vertical cut around PC1 = 0) could reduce the overlap but would not perfectly separate the two classes. Many points from Class A extend into the positive PC1 region, and several Class B points extend into the negative PC1 side.  </p> <p>Thus, the data is not strictly linearly separable. Any linear classifier (such as a perceptron or logistic regression with a single decision boundary) would inevitably misclassify points in the overlapping zone.</p> <p>This type of structure illustrates why simple linear models are insufficient. To capture the curved and complex boundary that better separates the two distributions, we require:</p> <ul> <li>Non-linear activation functions (e.g., tanh, ReLU), which allow the network to learn more flexible boundaries.  </li> <li>Multiple layers, enabling the model to progressively transform the feature space into one where the classes become more separable.  </li> </ul> <p>In other words, a multi-layer neural network with non-linearities is more appropriate for this problem, since it can learn decision regions that are not restricted to simple linear cuts.</p>"},{"location":"exercicios/ex1-data/main/#exercise-3-preparing-real-world-data-for-a-neural-network","title":"Exercise 3 \u2014 Preparing Real-World Data for a Neural Network","text":""},{"location":"exercicios/ex1-data/main/#1-describe-the-data","title":"1) Describe the Data","text":"<p>The dataset comes from the Kaggle competition Spaceship Titanic. Its main objective is to predict whether a passenger was transported to another dimension after the collision of the spaceship Titanic with a spacetime anomaly.  </p> <ul> <li>The target variable is <code>Transported</code>, a boolean column:</li> <li><code>True</code> \u2192 the passenger was transported.</li> <li><code>False</code> \u2192 the passenger was not transported.</li> </ul> <p>The dataset contains 14 columns (including the target). They can be grouped as follows:</p> <ul> <li>Numerical features (continuous values):</li> <li><code>Age</code> \u2192 passenger\u2019s age.</li> <li><code>RoomService</code> \u2192 amount spent in the room service.</li> <li><code>FoodCourt</code> \u2192 amount spent in the food court.</li> <li><code>ShoppingMall</code> \u2192 amount spent in the shopping mall.</li> <li><code>Spa</code> \u2192 amount spent in the spa.</li> <li> <p><code>VRDeck</code> \u2192 amount spent in the VR deck.</p> </li> <li> <p>Categorical features:</p> </li> <li><code>PassengerId</code> \u2192 unique passenger identifier.</li> <li><code>HomePlanet</code> \u2192 the planet where the passenger came from.</li> <li><code>CryoSleep</code> \u2192 whether the passenger was in cryogenic sleep (True/False).</li> <li><code>Cabin</code> \u2192 cabin identifier (deck/number/side).</li> <li><code>Destination</code> \u2192 destination planet.</li> <li><code>VIP</code> \u2192 whether the passenger is a VIP customer (True/False).</li> <li><code>Name</code> \u2192 passenger\u2019s name.</li> </ul> <p>To better understand the dataset, we analyzed the presence of missing values in each column. The following table shows the number of missing entries per feature:</p> <ul> <li>HomePlanet: 201  </li> <li>CryoSleep: 217  </li> <li>Cabin: 199  </li> <li>Destination: 182  </li> <li>Age: 179  </li> <li>VIP: 203  </li> <li>RoomService: 181  </li> <li>FoodCourt: 183  </li> <li>ShoppingMall: 208  </li> <li>Spa: 183  </li> <li>VRDeck: 188  </li> <li>Name: 200  </li> </ul> <p>Columns PassengerId and Transported contain no missing values.</p> <p>This confirms that several categorical (e.g., <code>HomePlanet</code>, <code>CryoSleep</code>) and numerical features (e.g., <code>Age</code>, <code>RoomService</code>) require preprocessing strategies to handle missing values before feeding the dataset into a neural network.</p> Code to Count Missing Values <pre><code>import pandas as pd\n\n# Load dataset\ndf = pd.read_csv(\"train.csv\")\n\n# Count missing values per column\nprint(\"Missing values per column:\")\nprint(df.isnull().sum())\n</code></pre>"},{"location":"exercicios/ex1-data/main/#2-preprocess-the-data","title":"2) Preprocess the Data","text":"<ul> <li>Numerical features (<code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>)   \u2192 Median imputation. Why: the median is robust to outliers (e.g., a few passengers with very high spending). Filling with median avoids biasing the distribution upwards like the mean would.</li> </ul> Code \u2014 Median imputation for numerical features <pre><code>import numpy as np\n\n# Numerical features\nnum_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n\ndf_num = df.copy()\nfor col in num_cols:\n    median_val = df_num[col].median()\n    df_num[col] = df_num[col].fillna(median_val)\n    print(f\"{col}: filled NaNs with median = {median_val}\")\n</code></pre> <ul> <li>Categorical features (<code>HomePlanet</code>, <code>CryoSleep</code>, <code>Destination</code>, <code>VIP</code>)   \u2192 Most frequent (mode) imputation. Why: keeps the dataset consistent by filling with the most common category, while not creating artificial new classes.</li> </ul> Code \u2014 Mode imputation for categorical features <pre><code>cat_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\"]\n\ndf_cat = df_num.copy()\nfor col in cat_cols:\n    mode_val = df_cat[col].mode(dropna=True)[0]\n    df_cat[col] = df_cat[col].fillna(mode_val)\n    print(f\"{col}: filled NaNs with mode = {repr(mode_val)}\")\n</code></pre> <ul> <li>Cabin   \u2192 Split into 3 separate features: <code>Deck</code>, <code>CabinNum</code>, and <code>Side</code>.  </li> <li><code>Deck</code>: letter at the start.  </li> <li><code>CabinNum</code>: numeric part (converted to integer).  </li> <li><code>Side</code>: last letter (e.g., P or S).   After splitting, apply mode imputation to each of the three. Why: splitting provides interpretable components instead of treating \u201cCabin\u201d as a single string.</li> </ul> Code \u2014 Split Cabin and impute <pre><code>df_cabin = df_cat.copy()\n\n# Split Cabin into 3 parts\ncabin_split = df_cabin[\"Cabin\"].str.split(\"/\", expand=True)\ndf_cabin[\"Deck\"] = cabin_split[0]\ndf_cabin[\"CabinNum\"] = pd.to_numeric(cabin_split[1], errors=\"coerce\")\ndf_cabin[\"Side\"] = cabin_split[2]\n\n# Drop original Cabin\ndf_cabin = df_cabin.drop(columns=[\"Cabin\"])\n\n# Impute Deck and Side with mode, CabinNum with median\nfor col in [\"Deck\", \"Side\"]:\n    mode_val = df_cabin[col].mode(dropna=True)[0]\n    df_cabin[col] = df_cabin[col].fillna(mode_val)\n    print(f\"{col}: filled NaNs with mode = {repr(mode_val)}\")\n\ncabin_num_median = df_cabin[\"CabinNum\"].median()\ndf_cabin[\"CabinNum\"] = df_cabin[\"CabinNum\"].fillna(cabin_num_median)\nprint(f\"CabinNum: filled NaNs with median = {cabin_num_median}\")\n</code></pre> <p>After handling missing values in the previous step, our categorical features include:</p> <ul> <li><code>HomePlanet</code> </li> <li><code>CryoSleep</code> </li> <li><code>Destination</code> </li> <li><code>VIP</code> </li> <li><code>Deck</code> </li> <li><code>Side</code></li> </ul> <p>(Note: <code>Cabin</code> was already split into <code>Deck</code>, <code>CabinNum</code>, <code>Side</code>.)</p> <p>We will use <code>pandas.get_dummies()</code> for simplicity. It automatically creates new columns for each category.</p> Code \u2014 One-hot encoding with pandas <pre><code># Assume df_clean is the dataset after missing value handling\ncategorical_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Deck\", \"Side\"]\n\n# One-hot encode categorical columns\ndf_encoded = pd.get_dummies(df_clean, columns=categorical_cols, drop_first=False)\n\nprint(\"Shape after encoding:\", df_encoded.shape)\nprint(\"Sample columns:\", df_encoded.columns[:15].tolist())\n</code></pre> <ul> <li>We use <code>drop_first=False</code> to keep all categories, ensuring no information loss.  </li> <li>This results in new binary columns like <code>HomePlanet_Earth</code>, <code>HomePlanet_Europa</code>, <code>HomePlanet_Mars</code>, etc.</li> </ul> <p>From the dataset (after missing-value handling and encoding), the main numerical columns are:</p> <ul> <li>Demographic &amp; cabin info: <code>Age</code>, <code>CabinNum</code> </li> <li>Spending features: <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code></li> </ul> Code \u2014 Normalize numerical features <pre><code>import numpy as np\n\n# Numeric columns\nnumeric_cols = [\"Age\", \"CabinNum\", \"RoomService\", \"FoodCourt\",\n                \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n\n# Copy DataFrame\ndf_normalized = df_encoded.copy()\n\n# Apply Min-Max normalization to [-1, 1]\nfor col in numeric_cols:\n    col_min = df_normalized[col].min()\n    col_max = df_normalized[col].max()\n\n    df_normalized[col] = 2 * (df_normalized[col] - col_min) / (col_max - col_min) - 1\n\n    print(f\"{col}: min={df_normalized[col].min()}, max={df_normalized[col].max()}\")\n</code></pre> <p>Normalizing inputs to the range [-1, 1] aligns them with the natural output of the <code>tanh</code> activation, which is zero-centered. This keeps data in the central, high-slope region of the function, avoiding saturation near -1 or +1 where gradients vanish. It also balances features on the same scale, preventing large-valued variables from dominating smaller ones. As a result, training becomes more stable and convergence is faster when using <code>tanh</code>.</p>"},{"location":"exercicios/ex1-data/main/#3-visualization-histograms-beforeafter-scaling","title":"3) Visualization \u2014 Histograms Before/After Scaling","text":"<p>Below we compare the distributions before and after normalization for two numerical features: <code>Age</code> and <code>CabinNum</code>.  </p> <p></p> Code \u2014 Histograms (before vs. after normalization) <pre><code>import matplotlib.pyplot as plt\n\n# Expect these DataFrames from your pipeline:\n# df_encoded  -&gt; before normalization\n# df_normalized -&gt; after normalization (numeric in [-1, 1])\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8), constrained_layout=True)\n\n# ---- Age ----\naxes[0, 0].hist(df_encoded[\"Age\"], bins=30, color=\"red\", alpha=0.6, edgecolor=\"black\")\naxes[0, 0].set_title(\"Age \u2014 Before Normalization\")\naxes[0, 0].set_xlabel(\"Value\"); axes[0, 0].set_ylabel(\"Frequency\"); axes[0, 0].grid(alpha=0.25)\n\naxes[0, 1].hist(df_normalized[\"Age\"], bins=30, color=\"blue\", alpha=0.6, edgecolor=\"black\")\naxes[0, 1].set_title(\"Age \u2014 After Normalization\")\naxes[0, 1].set_xlabel(\"Value\"); axes[0, 1].set_ylabel(\"Frequency\"); axes[0, 1].grid(alpha=0.25)\naxes[0, 1].set_xlim(-1, 1)\n\n# Match y-scale for Age row\nymax_age = max(axes[0, 0].get_ylim()[1], axes[0, 1].get_ylim()[1])\naxes[0, 0].set_ylim(0, ymax_age); axes[0, 1].set_ylim(0, ymax_age)\n\n# ---- CabinNum ----\naxes[1, 0].hist(df_encoded[\"CabinNum\"], bins=30, color=\"red\", alpha=0.6, edgecolor=\"black\")\naxes[1, 0].set_title(\"CabinNum \u2014 Before Normalization\")\naxes[1, 0].set_xlabel(\"Value\"); axes[1, 0].set_ylabel(\"Frequency\"); axes[1, 0].grid(alpha=0.25)\n\naxes[1, 1].hist(df_normalized[\"CabinNum\"], bins=30, color=\"blue\", alpha=0.6, edgecolor=\"black\")\naxes[1, 1].set_title(\"CabinNum \u2014 After Normalization\")\naxes[1, 1].set_xlabel(\"Value\"); axes[1, 1].set_ylabel(\"Frequency\"); axes[1, 1].grid(alpha=0.25)\naxes[1, 1].set_xlim(-1, 1)\n\n# Match y-scale for CabinNum row\nymax_cabin = max(axes[1, 0].get_ylim()[1], axes[1, 1].get_ylim()[1])\naxes[1, 0].set_ylim(0, ymax_cabin); axes[1, 1].set_ylim(0, ymax_cabin)\n\nfig.suptitle(\"Distributions Before vs After Normalization\", fontsize=14, y=1.02)\nplt.savefig(\"histograms_age_cabinnum_before_after_simple.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/","title":"2. Perceptron","text":""},{"location":"exercicios/ex2-perceptron/main/#2-perceptron","title":"2. Perceptron","text":""},{"location":"exercicios/ex2-perceptron/main/#exercise-1","title":"Exercise 1","text":""},{"location":"exercicios/ex2-perceptron/main/#1-data-generation","title":"1) Data Generation","text":"<p>We generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. The parameters are:</p> <ul> <li> <p>Class 0:   Mean = [1.5, 1.5]   Covariance matrix = [[0.5, 0], [0, 0.5]]</p> </li> <li> <p>Class 1:   Mean = [5, 5]   Covariance matrix = [[0.5, 0], [0, 0.5]]</p> </li> </ul> <p>These settings ensure that the two classes are mostly linearly separable, since the means are far apart and the variance is small, causing minimal overlap.</p> Code \u2014 Data Generation <pre><code>import numpy as np\n\n# Parameters for Class 0\nmean_class0 = [1.5, 1.5]\ncov_class0 = [[0.5, 0], [0, 0.5]]\n\n# Parameters for Class 1\nmean_class1 = [5, 5]\ncov_class1 = [[0.5, 0], [0, 0.5]]\n\n# Number of samples per class\nn_samples = 1000\n\n# Generate data\nclass0 = np.random.multivariate_normal(mean_class0, cov_class0, n_samples)\nclass1 = np.random.multivariate_normal(mean_class1, cov_class1, n_samples)\n\n# Combine into dataset\nX = np.vstack((class0, class1))\ny = np.hstack((np.zeros(n_samples), np.ones(n_samples)))\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#2-plot-the-data","title":"2) Plot the Data","text":"<p>We now plot the generated dataset to visualize the separation between the two classes. Each point is colored according to its class: blue for Class 0 and red for Class 1.</p> Code \u2014 Scatter Plot <pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(7, 7))\nplt.scatter(class0[:, 0], class0[:, 1], c=\"blue\", alpha=0.5, label=\"Class 0\")\nplt.scatter(class1[:, 0], class1[:, 1], c=\"red\", alpha=0.5, label=\"Class 1\")\nplt.legend()\nplt.title(\"Exercise 1 \u2014 Generated Data (Class 0 vs Class 1)\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>Figure 1 \u2014 Scatter plot of the two generated classes. Class 0 (blue) is centered at (1.5, 1.5), while Class 1 (red) is centered at (5, 5). The two distributions are clearly separated, confirming linear separability.</p>"},{"location":"exercicios/ex2-perceptron/main/#3-perceptron-implementation","title":"3) Perceptron Implementation","text":""},{"location":"exercicios/ex2-perceptron/main/#a-model-parameters","title":"a. Model Parameters","text":"<p>At its core, the perceptron keeps two types of parameters:</p> <ul> <li>Weights (<code>w</code>) \u2192 control the importance of each input feature.  </li> <li>Bias (<code>b</code>) \u2192 shifts the decision boundary away from the origin.  </li> </ul> <p>Together, they form a linear equation:</p> \\[ z = w_1 \\cdot x_1 + w_2 \\cdot x_2 + b \\] <p>If \\(z \\geq 0\\), the perceptron predicts Class 1. If \\(z &lt; 0\\), it predicts Class 0.</p> Code \u2014 Initialization <pre><code>class Perceptron:\n    def __init__(self, input_dim, learning_rate=0.01, max_epochs=100):\n        self.w = np.zeros(input_dim)   # weights\n        self.b = 0.0                   # bias\n        self.lr = learning_rate\n        self.max_epochs = max_epochs\n        self.accuracy_history = []\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#b-activation-function","title":"b. Activation Function","text":"<p>The perceptron uses a step function as its activation:  </p> <ul> <li>If the input is positive \u2192 output = 1.  </li> <li>Otherwise \u2192 output = 0.  </li> </ul> Code \u2014 Activation <pre><code>    def activation(self, z):\n        return 1 if z &gt;= 0 else 0\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#c-making-predictions","title":"c. Making Predictions","text":"<p>To classify a new point, the perceptron first computes the weighted sum of the inputs plus the bias:</p> \\[ z = w_1 \\cdot x_1 + w_2 \\cdot x_2 + b \\] <p>Then, it applies the step activation function:</p> \\[ \\hat{y} = \\begin{cases} 1, &amp; \\text{if } z \\geq 0 \\\\ 0, &amp; \\text{if } z &lt; 0 \\end{cases} \\] Code \u2014 Prediction <pre><code>    def predict(self, X):\n        z = np.dot(X, self.w) + self.b\n        return np.where(z &gt;= 0, 1, 0)\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#d-learning-rule-training","title":"d. Learning Rule (Training)","text":"<p>The perceptron learns by trial and error:</p> <ol> <li>Start with weights = 0 and bias = 0.  </li> <li>For each training example:<ul> <li>Compute the prediction.  </li> <li>Compare it to the true label.  </li> <li>If wrong, nudge the line by updating weights and bias.  </li> </ul> </li> </ol> <p>Update rule:</p> \\[ w_i \\leftarrow w_i + \\eta \\cdot (y - \\hat{y}) \\cdot x_i \\quad  \\] \\[ b \\leftarrow b + \\eta \\cdot (y - \\hat{y}) \\] <p>Where:</p> <ul> <li>\\(y\\) = true label  </li> <li>\\(\\hat{y}\\) = predicted label  </li> <li>\\(\\eta\\) = learning rate  </li> </ul> <p>Over time, the line moves so that it better separates the classes.</p> Code \u2014 Training <pre><code>    def fit(self, X, y):\n        for epoch in range(self.max_epochs):\n            errors = 0\n            for xi, target in zip(X, y):\n                prediction = self.activation(np.dot(xi, self.w) + self.b)\n                error = target - prediction\n                if error != 0:  # misclassified\n                    self.w += self.lr * error * xi\n                    self.b += self.lr * error\n                    errors += 1\n\n            # Track accuracy\n            predictions = self.predict(X)\n            accuracy = np.mean(predictions == y)\n            self.accuracy_history.append(accuracy)\n\n            if errors == 0:\n                print(f\"Converged after {epoch+1} epochs.\")\n                break\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#e-evaluating-the-model","title":"e. Evaluating the Model","text":"<p>Once trained, the perceptron can be evaluated by checking how many points it classifies correctly (accuracy).</p> Code \u2014 Score Function <pre><code>    def score(self, X, y):\n        predictions = self.predict(X)\n        return np.mean(predictions == y)\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#f-training-on-our-data","title":"f. Training on Our Data","text":"<p>We now train the perceptron on the dataset generated earlier. Since the data is linearly separable, we expect the perceptron to converge quickly and reach 100% accuracy.</p> Code \u2014 Running the Model <pre><code>perceptron = Perceptron(input_dim=2, learning_rate=0.01, max_epochs=100)\nperceptron.fit(X, y)\n\nfinal_accuracy = perceptron.score(X, y)\nprint(\"Final Weights:\", perceptron.w)\nprint(\"Final Bias:\", perceptron.b)\nprint(\"Final Accuracy:\", final_accuracy)\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#4-results-and-conclusions","title":"4) Results and Conclusions","text":"<p>After training the perceptron on the linearly separable dataset, we obtained the following results:</p> <ul> <li>Converged after 10 epochs </li> <li>Final Weights: \\([0.0236, \\; 0.0219]\\) </li> <li>Final Bias: \\(-0.16\\) </li> <li>Final Accuracy: 100%  </li> </ul>"},{"location":"exercicios/ex2-perceptron/main/#a-decision-boundary","title":"a. Decision Boundary","text":"<p>The decision boundary is defined by the linear equation:</p> \\[ w_1 \\cdot x_1 + w_2 \\cdot x_2 + b = 0 \\] <p>Substituting the final parameters:</p> \\[ 0.0236 \\cdot x_1 + 0.0219 \\cdot x_2 - 0.16 = 0 \\] <p>This line divides the 2D space into two regions:</p> <ul> <li>Points classified as Class 0 (blue) when \\(z &lt; 0\\).  </li> <li>Points classified as Class 1 (red) when \\(z \\geq 0\\).  </li> </ul> Code \u2014 Plot Decision Boundary <pre><code>def plot_decision_boundary(X, y, model):\n  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n  xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                      np.linspace(y_min, y_max, 200))\n  grid = np.c_[xx.ravel(), yy.ravel()]\n  Z = model.predict(grid).reshape(xx.shape)\n\n  # Use bwr colormap so 0=blue, 1=red\n  plt.contourf(xx, yy, Z, alpha=0.2, cmap=plt.cm.bwr)\n\n  plt.scatter(X[y==0][:, 0], X[y==0][:, 1], c=\"blue\", label=\"Class 0\", alpha=0.5)\n  plt.scatter(X[y==1][:, 0], X[y==1][:, 1], c=\"red\", label=\"Class 1\", alpha=0.5)\n\n  # Highlight misclassified points\n  predictions = model.predict(X)\n  misclassified = X[predictions != y]\n  if len(misclassified) &gt; 0:\n      plt.scatter(misclassified[:, 0], misclassified[:, 1],\n                  c=\"black\", marker=\"x\", label=\"Misclassified\")\n\n  plt.legend()\n  plt.title(\"Decision Boundary with Data Points\")\n  plt.xlabel(\"x1\")\n  plt.ylabel(\"x2\")\n  plt.show()\n</code></pre> <p></p> <p>Figure 2 \u2014 Final decision boundary learned by the perceptron. The separation line clearly divides Class 0 (blue) and Class 1 (red).</p>"},{"location":"exercicios/ex2-perceptron/main/#b-accuracy-over-epochs","title":"b. Accuracy over Epochs","text":"<p>During training, the perceptron progressively adjusted the weights and bias, improving classification accuracy at each epoch. Convergence was reached at epoch 10, after which no further updates were necessary.</p> Code \u2014 Accuracy Plot <pre><code>plt.plot(range(1, len(perceptron.accuracy_history)+1),\n         perceptron.accuracy_history, marker=\"o\")\nplt.title(\"Training Accuracy over Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n</code></pre> <p></p> <p>Figure 3 \u2014 Accuracy progression over epochs. The perceptron starts near random guessing (~50%) and quickly converges to 100% accuracy by epoch 10.</p>"},{"location":"exercicios/ex2-perceptron/main/#c-discussion","title":"c. Discussion","text":"<p>The results highlight the fundamental properties of the perceptron:</p> <p>The perceptron converged after only 10 epochs, reaching 100% accuracy with final weights  [ 0.0236 , 0.0219 ] [0.0236,0.0219] and bias  \u2212 0.16 \u22120.16. This quick convergence happens because the dataset is linearly separable: a straight line is sufficient to perfectly divide the two classes. In such cases, the perceptron learning rule guarantees convergence in a finite number of steps, and the updates quickly adjust the decision boundary to separate the clusters without errors.</p>"},{"location":"exercicios/ex2-perceptron/main/#exercise-2","title":"Exercise 2","text":""},{"location":"exercicios/ex2-perceptron/main/#1-data-generation_1","title":"1) Data Generation","text":"<p>We generate two classes of 2D data points (1000 samples each) using multivariate normal distributions.  </p> <ul> <li> <p>Class 0:   Mean = [3, 3]   Covariance matrix = [[1.5, 0], [0, 1.5]]</p> </li> <li> <p>Class 1:   Mean = [4, 4]   Covariance matrix = [[1.5, 0], [0, 1.5]]</p> </li> </ul> Code \u2014 Data Generation <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nmean_class0 = [3, 3]\ncov_class0 = [[1.5, 0], [0, 1.5]]\n\nmean_class1 = [4, 4]\ncov_class1 = [[1.5, 0], [0, 1.5]]\n\nn_samples = 1000\nclass0 = np.random.multivariate_normal(mean_class0, cov_class0, n_samples)\nclass1 = np.random.multivariate_normal(mean_class1, cov_class1, n_samples)\n\nX2 = np.vstack((class0, class1))\ny2 = np.hstack((np.zeros(n_samples), np.ones(n_samples)))\n\nplt.figure(figsize=(7, 7))\nplt.scatter(class0[:, 0], class0[:, 1], c=\"blue\", alpha=0.5, label=\"Class 0\")\nplt.scatter(class1[:, 0], class1[:, 1], c=\"red\", alpha=0.5, label=\"Class 1\")\nplt.legend()\nplt.title(\"Exercise 2 - Generated Data (Class 0 vs Class 1)\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>Figure 1 \u2014 Generated dataset. Strong overlap exists between Class 0 (blue) and Class 1 (red).</p>"},{"location":"exercicios/ex2-perceptron/main/#2-perceptron-training","title":"2) Perceptron Training","text":"<p>We reuse the same perceptron implementation from Exercise 1.  </p> Code \u2014 Training <pre><code>perceptron2 = Perceptron(input_dim=2, learning_rate=0.01, max_epochs=100)\nperceptron2.fit(X2, y2)\n\nfinal_accuracy2 = perceptron2.score(X2, y2)\nprint(\"Final Weights:\", perceptron2.w)\nprint(\"Final Bias:\", perceptron2.b)\nprint(\"Final Accuracy:\", final_accuracy2)\n</code></pre>"},{"location":"exercicios/ex2-perceptron/main/#3-results","title":"3) Results","text":"<p>We visualize the decision boundary and training accuracy.</p> Code \u2014 Decision Boundary <pre><code>plot_decision_boundary(X2, y2, perceptron2)\n</code></pre> <p> Figure 2 \u2014 Decision boundary found by the perceptron. Misclassified points are visible due to overlap.</p> Code \u2014 Accuracy over Epochs <pre><code>plt.plot(range(1, len(perceptron2.accuracy_history)+1),\n         perceptron2.accuracy_history, marker=\"o\")\nplt.title(\"Training Accuracy over Epochs (Exercise 2)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.grid(True)\nplt.show()\n</code></pre> <p> Figure 3 \u2014 Accuracy progression over 100 epochs. The model oscillates around ~50%, never reaching full convergence.</p>"},{"location":"exercicios/ex2-perceptron/main/#4-conclusion","title":"4) Conclusion","text":"<p>The perceptron was trained on a dataset where the two classes present significant overlap. As a result, the algorithm was unable to find a linear decision boundary capable of perfectly separating the points. The decision boundary obtained (Figure 2) shows that many samples from both classes remain in the wrong region, confirming that misclassifications are unavoidable in this scenario. This behavior is also reflected in the training accuracy curve (Figure 3), which fluctuates around 50% and does not stabilize, indicating that the model could not effectively learn a separation rule. Since no epoch achieved a state without errors, the algorithm did not converge at any point during training, reaching the maximum of 100 epochs without success. These results reinforce one of the fundamental limitations of the perceptron: it only converges when the data is linearly separable. In problems where the distributions of the classes overlap, as in this exercise, the perceptron oscillates indefinitely and its accuracy remains close to random guessing. </p>"},{"location":"exercicios/ex3-mlp/main/","title":"Main","text":""},{"location":"exercicios/ex3-mlp/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"exercicios/ex3-mlp/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"exercicios/ex3-mlp/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"exercicios/ex3-mlp/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"},{"location":"exercicios/ex4-metrics/main/","title":"Main","text":""},{"location":"exercicios/ex4-metrics/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"exercicios/ex4-metrics/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"exercicios/ex4-metrics/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"exercicios/ex4-metrics/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"},{"location":"projeto/main/","title":"Projects","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"}]}